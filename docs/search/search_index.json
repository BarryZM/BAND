{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BAND\uff1aBERT Application aNd Deployment Simple and efficient BERT model development and deployment, \u7b80\u5355\u9ad8\u6548\u7684 BERT \u6a21\u578b\u5f00\u53d1\u548c\u90e8\u7f72 BAND BAND\uff1aBERT Application aNd Deployment \u63a2\u7d22\u672c\u9879\u76ee\u7684\u6587\u6863 \u00bb \u67e5\u770bDemo \u00b7 \u62a5\u544aBug \u00b7 \u63d0\u51fa\u65b0\u7279\u6027 \u00b7 \u95ee\u9898\u4ea4\u6d41 \u76ee\u5f55 \u4e0a\u624b\u6307\u5357 \u5f00\u53d1\u524d\u7684\u914d\u7f6e\u8981\u6c42 \u5b89\u88c5\u6b65\u9aa4 \u6587\u4ef6\u76ee\u5f55\u8bf4\u660e \u5f00\u53d1\u7684\u67b6\u6784 \u90e8\u7f72 \u4f7f\u7528\u5230\u7684\u6846\u67b6 \u8d21\u732e\u8005 \u5982\u4f55\u53c2\u4e0e\u5f00\u6e90\u9879\u76ee \u7248\u672c\u63a7\u5236 \u4f5c\u8005 \u9e23\u8c22 \u4e0a\u624b\u6307\u5357 \u5f00\u53d1\u524d\u7684\u914d\u7f6e\u8981\u6c42 xxxxx x.x.x xxxxx x.x.x \u5b89\u88c5\u6b65\u9aa4 Clone the repo git clone https://github.com/SunYanCN/BAND.git \u6587\u4ef6\u76ee\u5f55\u8bf4\u660e filetree \u251c\u2500\u2500 ARCHITECTURE.md \u251c\u2500\u2500 LICENSE.txt \u251c\u2500\u2500 README.md \u251c\u2500\u2500 /account/ \u251c\u2500\u2500 /bbs/ \u251c\u2500\u2500 /docs/ \u2502 \u251c\u2500\u2500 /rules/ \u2502 \u2502 \u251c\u2500\u2500 backend.txt \u2502 \u2502 \u2514\u2500\u2500 frontend.txt \u251c\u2500\u2500 manage.py \u251c\u2500\u2500 /oa/ \u251c\u2500\u2500 /static/ \u251c\u2500\u2500 /templates/ \u251c\u2500\u2500 useless.md \u2514\u2500\u2500 /util/ \u90e8\u7f72 \u6682\u65e0 \u4f7f\u7528\u5230\u7684\u6846\u67b6 TensorFlow simple-tensorflow-serving \u4f5c\u8005 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u8054\u7cfb\u6211\uff1a - Email : sunyanhust@163.com - NLP\u6280\u672fQQ\u4ea4\u6d41\u7fa4 \uff1a859886087 \u60a8\u4e5f\u53ef\u4ee5\u5728\u8d21\u732e\u8005\u540d\u5355\u4e2d\u53c2\u770b\u6240\u6709\u53c2\u4e0e\u8be5\u9879\u76ee\u7684\u5f00\u53d1\u8005\u3002 \u8d21\u732e\u8005 \u8bf7\u9605\u8bfb CONTRIBUTING.md \u67e5\u9605\u4e3a\u8be5\u9879\u76ee\u505a\u51fa\u8d21\u732e\u7684\u5f00\u53d1\u8005\u3002 \u5982\u4f55\u53c2\u4e0e\u5f00\u6e90\u9879\u76ee \u8d21\u732e\u4f7f\u5f00\u6e90\u793e\u533a\u6210\u4e3a\u4e00\u4e2a\u5b66\u4e60\u3001\u6fc0\u52b1\u548c\u521b\u9020\u7684\u7edd\u4f73\u573a\u6240\u3002\u4f60\u6240\u4f5c\u7684\u4efb\u4f55\u8d21\u732e\u90fd\u662f \u975e\u5e38\u611f\u8c22 \u7684\u3002 Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request \u7248\u6743\u8bf4\u660e \u8be5\u9879\u76ee\u7b7e\u7f72\u4e86MIT \u6388\u6743\u8bb8\u53ef\uff0c\u8be6\u60c5\u8bf7\u53c2\u9605 LICENSE \u7248\u672c\u63a7\u5236 \u8be5\u9879\u76ee\u4f7f\u7528Git\u8fdb\u884c\u7248\u672c\u7ba1\u7406\u3002\u60a8\u53ef\u4ee5\u5728repository\u53c2\u770b\u5f53\u524d\u53ef\u7528\u7248\u672c\u3002 \u9e23\u8c22 Free Logo Design Headliner","title":"Home"},{"location":"#bandbert-application-and-deployment","text":"Simple and efficient BERT model development and deployment, \u7b80\u5355\u9ad8\u6548\u7684 BERT \u6a21\u578b\u5f00\u53d1\u548c\u90e8\u7f72","title":"BAND\uff1aBERT Application aNd Deployment"},{"location":"#_1","text":"\u4e0a\u624b\u6307\u5357 \u5f00\u53d1\u524d\u7684\u914d\u7f6e\u8981\u6c42 \u5b89\u88c5\u6b65\u9aa4 \u6587\u4ef6\u76ee\u5f55\u8bf4\u660e \u5f00\u53d1\u7684\u67b6\u6784 \u90e8\u7f72 \u4f7f\u7528\u5230\u7684\u6846\u67b6 \u8d21\u732e\u8005 \u5982\u4f55\u53c2\u4e0e\u5f00\u6e90\u9879\u76ee \u7248\u672c\u63a7\u5236 \u4f5c\u8005 \u9e23\u8c22","title":"\u76ee\u5f55"},{"location":"#_2","text":"","title":"\u4e0a\u624b\u6307\u5357"},{"location":"#_3","text":"xxxxx x.x.x xxxxx x.x.x","title":"\u5f00\u53d1\u524d\u7684\u914d\u7f6e\u8981\u6c42"},{"location":"#_4","text":"Clone the repo git clone https://github.com/SunYanCN/BAND.git","title":"\u5b89\u88c5\u6b65\u9aa4"},{"location":"#_5","text":"filetree \u251c\u2500\u2500 ARCHITECTURE.md \u251c\u2500\u2500 LICENSE.txt \u251c\u2500\u2500 README.md \u251c\u2500\u2500 /account/ \u251c\u2500\u2500 /bbs/ \u251c\u2500\u2500 /docs/ \u2502 \u251c\u2500\u2500 /rules/ \u2502 \u2502 \u251c\u2500\u2500 backend.txt \u2502 \u2502 \u2514\u2500\u2500 frontend.txt \u251c\u2500\u2500 manage.py \u251c\u2500\u2500 /oa/ \u251c\u2500\u2500 /static/ \u251c\u2500\u2500 /templates/ \u251c\u2500\u2500 useless.md \u2514\u2500\u2500 /util/","title":"\u6587\u4ef6\u76ee\u5f55\u8bf4\u660e"},{"location":"#_6","text":"\u6682\u65e0","title":"\u90e8\u7f72"},{"location":"#_7","text":"TensorFlow simple-tensorflow-serving","title":"\u4f7f\u7528\u5230\u7684\u6846\u67b6"},{"location":"#_8","text":"\u60a8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u8054\u7cfb\u6211\uff1a - Email : sunyanhust@163.com - NLP\u6280\u672fQQ\u4ea4\u6d41\u7fa4 \uff1a859886087 \u60a8\u4e5f\u53ef\u4ee5\u5728\u8d21\u732e\u8005\u540d\u5355\u4e2d\u53c2\u770b\u6240\u6709\u53c2\u4e0e\u8be5\u9879\u76ee\u7684\u5f00\u53d1\u8005\u3002","title":"\u4f5c\u8005"},{"location":"#_9","text":"\u8bf7\u9605\u8bfb CONTRIBUTING.md \u67e5\u9605\u4e3a\u8be5\u9879\u76ee\u505a\u51fa\u8d21\u732e\u7684\u5f00\u53d1\u8005\u3002","title":"\u8d21\u732e\u8005"},{"location":"#_10","text":"\u8d21\u732e\u4f7f\u5f00\u6e90\u793e\u533a\u6210\u4e3a\u4e00\u4e2a\u5b66\u4e60\u3001\u6fc0\u52b1\u548c\u521b\u9020\u7684\u7edd\u4f73\u573a\u6240\u3002\u4f60\u6240\u4f5c\u7684\u4efb\u4f55\u8d21\u732e\u90fd\u662f \u975e\u5e38\u611f\u8c22 \u7684\u3002 Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request","title":"\u5982\u4f55\u53c2\u4e0e\u5f00\u6e90\u9879\u76ee"},{"location":"#_11","text":"\u8be5\u9879\u76ee\u7b7e\u7f72\u4e86MIT \u6388\u6743\u8bb8\u53ef\uff0c\u8be6\u60c5\u8bf7\u53c2\u9605 LICENSE","title":"\u7248\u6743\u8bf4\u660e"},{"location":"#_12","text":"\u8be5\u9879\u76ee\u4f7f\u7528Git\u8fdb\u884c\u7248\u672c\u7ba1\u7406\u3002\u60a8\u53ef\u4ee5\u5728repository\u53c2\u770b\u5f53\u524d\u53ef\u7528\u7248\u672c\u3002","title":"\u7248\u672c\u63a7\u5236"},{"location":"#_13","text":"Free Logo Design Headliner","title":"\u9e23\u8c22"},{"location":"CONTRIBUTING/","text":"Contribution Guide We welcome any contributions whether it's, Submitting feedback Fixing bugs Or implementing a new feature. Please read this guide before making any contributions. Submit Feedback The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels. Fix Bugs: You may look through the GitHub issues for bugs. Implement Features You may look through the GitHub issues for feature requests. Pull Requests (PR) Fork the repository and a create a new branch from the master branch. For bug fixes, add new tests and for new features please add changes to the documentation. Do a PR from your new branch to our master branch of the original Headliner repo. Documentation Make sure any new function or class you introduce has proper docstrings. Testing We use pytest for our testing. Make sure to write tests for any new feature and/or bug fixes. Main Contributor List We maintain a list of main contributors to appreciate all the contributions.","title":"Contribution"},{"location":"CONTRIBUTING/#contribution-guide","text":"We welcome any contributions whether it's, Submitting feedback Fixing bugs Or implementing a new feature. Please read this guide before making any contributions.","title":"Contribution Guide"},{"location":"CONTRIBUTING/#submit-feedback","text":"The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels.","title":"Submit Feedback"},{"location":"CONTRIBUTING/#fix-bugs","text":"You may look through the GitHub issues for bugs.","title":"Fix Bugs:"},{"location":"CONTRIBUTING/#implement-features","text":"You may look through the GitHub issues for feature requests.","title":"Implement Features"},{"location":"CONTRIBUTING/#pull-requests-pr","text":"Fork the repository and a create a new branch from the master branch. For bug fixes, add new tests and for new features please add changes to the documentation. Do a PR from your new branch to our master branch of the original Headliner repo.","title":"Pull Requests (PR)"},{"location":"CONTRIBUTING/#documentation","text":"Make sure any new function or class you introduce has proper docstrings.","title":"Documentation"},{"location":"CONTRIBUTING/#testing","text":"We use pytest for our testing. Make sure to write tests for any new feature and/or bug fixes.","title":"Testing"},{"location":"CONTRIBUTING/#main-contributor-list","text":"We maintain a list of main contributors to appreciate all the contributions.","title":"Main Contributor List"},{"location":"LICENSE/","text":"MIT License Copyright (c) 2019 SunYan Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"callbacks/","text":"class EvalCallBack __init__ def __init__ ( kash_model , valid_x , valid_y , step , batch_size , average ) Evaluate callback, calculate precision, recall and f1 Args: kash_model: the band model to evaluate valid_x: feature data valid_y: label data step: step, default 5 batch_size: batch size, default 256 on_epoch_end def on_epoch_end ( epoch , logs )","title":"Callbacks"},{"location":"callbacks/#class-evalcallback","text":"","title":"class EvalCallBack"},{"location":"callbacks/#9595init9595","text":"def __init__ ( kash_model , valid_x , valid_y , step , batch_size , average ) Evaluate callback, calculate precision, recall and f1 Args: kash_model: the band model to evaluate valid_x: feature data valid_y: label data step: step, default 5 batch_size: batch size, default 256","title":"__init__"},{"location":"callbacks/#on95epoch95end","text":"def on_epoch_end ( epoch , logs )","title":"on_epoch_end"},{"location":"corpus/","text":"class DataReader read_conll_format_file def read_conll_format_file ( file_path , text_index , label_index ) Read conll format data_file Args: file_path: path of target file text_index: index of text data, default 0 label_index: index of label data, default 1 class ChineseDailyNerCorpus Chinese Daily New New Corpus https://github.com/zjy-ucas/ChineseNER/ load_data def load_data ( cls , subset_name , shuffle ) Load dataset as sequence labeling format, char level tokenized Sample:: train_x, train_y = ChineseDailyNerCorpus.load_data('train') test_x, test_y = ChineseDailyNerCorpus.load_data('test') Args subset_name : {train, test, valid} shuffle : should shuffle or not, default True. Returns class CONLL2003ENCorpus load_data def load_data ( cls , subset_name , task_name , shuffle ) class SMP2018ECDTCorpus https://worksheets.codalab.org/worksheets/0x27203f932f8341b79841d50ce0fd684f/ This dataset is released by the Evaluation of Chinese Human-Computer Dialogue Technology (SMP2018-ECDT) task 1 and is provided by the iFLYTEK Corporation, which is a Chinese human-computer dialogue dataset. load_data def load_data ( cls , subset_name , shuffle , cutter ) Load dataset as sequence classification format, char level tokenized Samples:: train_x, train_y = SMP2018ECDTCorpus.load_data('train') test_x, test_y = SMP2018ECDTCorpus.load_data('test') Args subset_name : {train, test, valid} shuffle : should shuffle or not, default True. cutter : sentence cutter, {char, jieba} Returns","title":"Corpus"},{"location":"corpus/#class-datareader","text":"","title":"class DataReader"},{"location":"corpus/#read95conll95format95file","text":"def read_conll_format_file ( file_path , text_index , label_index ) Read conll format data_file Args: file_path: path of target file text_index: index of text data, default 0 label_index: index of label data, default 1","title":"read_conll_format_file"},{"location":"corpus/#class-chinesedailynercorpus","text":"Chinese Daily New New Corpus https://github.com/zjy-ucas/ChineseNER/","title":"class ChineseDailyNerCorpus"},{"location":"corpus/#load95data","text":"def load_data ( cls , subset_name , shuffle ) Load dataset as sequence labeling format, char level tokenized Sample:: train_x, train_y = ChineseDailyNerCorpus.load_data('train') test_x, test_y = ChineseDailyNerCorpus.load_data('test')","title":"load_data"},{"location":"corpus/#args","text":"subset_name : {train, test, valid} shuffle : should shuffle or not, default True.","title":"Args"},{"location":"corpus/#returns","text":"","title":"Returns"},{"location":"corpus/#class-conll2003encorpus","text":"","title":"class CONLL2003ENCorpus"},{"location":"corpus/#load95data_1","text":"def load_data ( cls , subset_name , task_name , shuffle )","title":"load_data"},{"location":"corpus/#class-smp2018ecdtcorpus","text":"https://worksheets.codalab.org/worksheets/0x27203f932f8341b79841d50ce0fd684f/ This dataset is released by the Evaluation of Chinese Human-Computer Dialogue Technology (SMP2018-ECDT) task 1 and is provided by the iFLYTEK Corporation, which is a Chinese human-computer dialogue dataset.","title":"class SMP2018ECDTCorpus"},{"location":"corpus/#load95data_2","text":"def load_data ( cls , subset_name , shuffle , cutter ) Load dataset as sequence classification format, char level tokenized Samples:: train_x, train_y = SMP2018ECDTCorpus.load_data('train') test_x, test_y = SMP2018ECDTCorpus.load_data('test')","title":"load_data"},{"location":"corpus/#args_1","text":"subset_name : {train, test, valid} shuffle : should shuffle or not, default True. cutter : sentence cutter, {char, jieba}","title":"Args"},{"location":"corpus/#returns_1","text":"","title":"Returns"},{"location":"macros/","text":"class TaskType class Config __init__ def __init__ () use_cudnn_cell def use_cudnn_cell () use_cudnn_cell def use_cudnn_cell ( value ) to_dict def to_dict ()","title":"Macros"},{"location":"macros/#class-tasktype","text":"","title":"class TaskType"},{"location":"macros/#class-config","text":"","title":"class Config"},{"location":"macros/#9595init9595","text":"def __init__ ()","title":"__init__"},{"location":"macros/#use95cudnn95cell","text":"def use_cudnn_cell ()","title":"use_cudnn_cell"},{"location":"macros/#use95cudnn95cell_1","text":"def use_cudnn_cell ( value )","title":"use_cudnn_cell"},{"location":"macros/#to95dict","text":"def to_dict ()","title":"to_dict"},{"location":"migeration/","text":"show_migration_guide def show_migration_guide ()","title":"Migeration"},{"location":"migeration/#show95migration95guide","text":"def show_migration_guide ()","title":"show_migration_guide"},{"location":"utils/","text":"unison_shuffled_copies def unison_shuffled_copies ( a , b ) get_list_subset def get_list_subset ( target , index_list ) custom_object_scope def custom_object_scope () load_model def load_model ( model_path , load_weights ) Load saved model from saved model from model.save function Args: model_path: model folder path load_weights: only load model structure and vocabulary when set to False, default True. load_processor def load_processor ( model_path ) Load processor from model When we using tf-serving, we need to use model's processor to pre-process data Args: model_path convert_to_saved_model def convert_to_saved_model ( model , model_path , version , inputs , outputs ) Export model for tensorflow serving Args: model: Target model model_path: The path to which the SavedModel will be stored. version: The model version code, default timestamp inputs: dict mapping string input names to tensors. These are added to the SignatureDef as the inputs. outputs: dict mapping string output names to tensors. These are added to the SignatureDef as the outputs.","title":"Utils"},{"location":"utils/#unison95shuffled95copies","text":"def unison_shuffled_copies ( a , b )","title":"unison_shuffled_copies"},{"location":"utils/#get95list95subset","text":"def get_list_subset ( target , index_list )","title":"get_list_subset"},{"location":"utils/#custom95object95scope","text":"def custom_object_scope ()","title":"custom_object_scope"},{"location":"utils/#load95model","text":"def load_model ( model_path , load_weights ) Load saved model from saved model from model.save function Args: model_path: model folder path load_weights: only load model structure and vocabulary when set to False, default True.","title":"load_model"},{"location":"utils/#load95processor","text":"def load_processor ( model_path ) Load processor from model When we using tf-serving, we need to use model's processor to pre-process data Args: model_path","title":"load_processor"},{"location":"utils/#convert95to95saved95model","text":"def convert_to_saved_model ( model , model_path , version , inputs , outputs ) Export model for tensorflow serving Args: model: Target model model_path: The path to which the SavedModel will be stored. version: The model version code, default timestamp inputs: dict mapping string input names to tensors. These are added to the SignatureDef as the inputs. outputs: dict mapping string output names to tensors. These are added to the SignatureDef as the outputs.","title":"convert_to_saved_model"},{"location":"version/","text":"","title":"Version"},{"location":"embeddings/bare_embedding/","text":"class BareEmbedding Embedding layer without pre-training, train embedding layer while training model __init__ def __init__ ( task , sequence_length , embedding_size , processor , from_saved_model ) Init bare embedding (embedding without pre-training) Args sequence_length : 'auto' , 'variable' or integer. When using 'auto' , use the 95% of corpus length as sequence length. When using 'variable' , model input shape will set to None, which can handle various length of input, it will use the length of max sequence in every batch for sequence length. If using an integer, let's say 50 , the input output sequence length will set to 50. embedding_size : Dimension of the dense embedding.","title":"bare_embedding"},{"location":"embeddings/bare_embedding/#class-bareembedding","text":"Embedding layer without pre-training, train embedding layer while training model","title":"class BareEmbedding"},{"location":"embeddings/bare_embedding/#9595init9595","text":"def __init__ ( task , sequence_length , embedding_size , processor , from_saved_model ) Init bare embedding (embedding without pre-training)","title":"__init__"},{"location":"embeddings/bare_embedding/#args","text":"sequence_length : 'auto' , 'variable' or integer. When using 'auto' , use the 95% of corpus length as sequence length. When using 'variable' , model input shape will set to None, which can handle various length of input, it will use the length of max sequence in every batch for sequence length. If using an integer, let's say 50 , the input output sequence length will set to 50. embedding_size : Dimension of the dense embedding.","title":"Args"},{"location":"embeddings/base_embedding/","text":"class Embedding Base class for Embedding Model info def info () __init__ def __init__ ( task , sequence_length , embedding_size , processor , from_saved_model ) token_count def token_count () corpus token count sequence_length def sequence_length () model sequence length label2idx def label2idx () label to index dict token2idx def token2idx () token to index dict tokenizer def tokenizer () sequence_length def sequence_length ( val ) analyze_corpus def analyze_corpus ( x , y ) Prepare embedding layer and pre-processor for labeling task Args x : y : embed_one def embed_one ( sentence ) Convert one sentence to vector Args sentence : target sentence, list of str Returns embed def embed ( sentence_list , debug ) batch embed sentences Args sentence_list : Sentence list to embed debug : show debug info Returns process_x_dataset def process_x_dataset ( data , subset ) batch process feature data while training Args data : target dataset subset : subset index list Returns process_y_dataset def process_y_dataset ( data , subset ) batch process labels data while training Args data : target dataset subset : subset index list Returns reverse_numerize_label_sequences def reverse_numerize_label_sequences ( sequences , lengths ) __repr__ def __repr__ () __str__ def __str__ ()","title":"base_embedding"},{"location":"embeddings/base_embedding/#class-embedding","text":"Base class for Embedding Model","title":"class Embedding"},{"location":"embeddings/base_embedding/#info","text":"def info ()","title":"info"},{"location":"embeddings/base_embedding/#9595init9595","text":"def __init__ ( task , sequence_length , embedding_size , processor , from_saved_model )","title":"__init__"},{"location":"embeddings/base_embedding/#token95count","text":"def token_count () corpus token count","title":"token_count"},{"location":"embeddings/base_embedding/#sequence95length","text":"def sequence_length () model sequence length","title":"sequence_length"},{"location":"embeddings/base_embedding/#label2idx","text":"def label2idx () label to index dict","title":"label2idx"},{"location":"embeddings/base_embedding/#token2idx","text":"def token2idx () token to index dict","title":"token2idx"},{"location":"embeddings/base_embedding/#tokenizer","text":"def tokenizer ()","title":"tokenizer"},{"location":"embeddings/base_embedding/#sequence95length_1","text":"def sequence_length ( val )","title":"sequence_length"},{"location":"embeddings/base_embedding/#analyze95corpus","text":"def analyze_corpus ( x , y ) Prepare embedding layer and pre-processor for labeling task","title":"analyze_corpus"},{"location":"embeddings/base_embedding/#args","text":"x : y :","title":"Args"},{"location":"embeddings/base_embedding/#embed95one","text":"def embed_one ( sentence ) Convert one sentence to vector","title":"embed_one"},{"location":"embeddings/base_embedding/#args_1","text":"sentence : target sentence, list of str","title":"Args"},{"location":"embeddings/base_embedding/#returns","text":"","title":"Returns"},{"location":"embeddings/base_embedding/#embed","text":"def embed ( sentence_list , debug ) batch embed sentences","title":"embed"},{"location":"embeddings/base_embedding/#args_2","text":"sentence_list : Sentence list to embed debug : show debug info","title":"Args"},{"location":"embeddings/base_embedding/#returns_1","text":"","title":"Returns"},{"location":"embeddings/base_embedding/#process95x95dataset","text":"def process_x_dataset ( data , subset ) batch process feature data while training","title":"process_x_dataset"},{"location":"embeddings/base_embedding/#args_3","text":"data : target dataset subset : subset index list","title":"Args"},{"location":"embeddings/base_embedding/#returns_2","text":"","title":"Returns"},{"location":"embeddings/base_embedding/#process95y95dataset","text":"def process_y_dataset ( data , subset ) batch process labels data while training","title":"process_y_dataset"},{"location":"embeddings/base_embedding/#args_4","text":"data : target dataset subset : subset index list","title":"Args"},{"location":"embeddings/base_embedding/#returns_3","text":"","title":"Returns"},{"location":"embeddings/base_embedding/#reverse95numerize95label95sequences","text":"def reverse_numerize_label_sequences ( sequences , lengths )","title":"reverse_numerize_label_sequences"},{"location":"embeddings/base_embedding/#9595repr9595","text":"def __repr__ ()","title":"__repr__"},{"location":"embeddings/base_embedding/#9595str9595","text":"def __str__ ()","title":"__str__"},{"location":"embeddings/bert_embedding/","text":"class BERTEmbedding Pre-trained BERT embedding info def info () __init__ def __init__ ( model_folder , layer_nums , trainable , task , sequence_length , processor , from_saved_model ) Args: task: model_folder: layer_nums: number of layers whose outputs will be concatenated into a single tensor, default 4 , output the last 4 hidden layers as the thesis suggested trainable: whether if the model is trainable, default False and set it to True for fine-tune this embedding layer during your training sequence_length: processor: from_saved_model analyze_corpus def analyze_corpus ( x , y ) Prepare embedding layer and pre-processor for labeling task Args x : y : embed def embed ( sentence_list , debug ) batch embed sentences Args sentence_list : Sentence list to embed debug : show debug log Returns process_x_dataset def process_x_dataset ( data , subset ) batch process feature data while training Args data : target dataset subset : subset index list Returns","title":"bert_embedding"},{"location":"embeddings/bert_embedding/#class-bertembedding","text":"Pre-trained BERT embedding","title":"class BERTEmbedding"},{"location":"embeddings/bert_embedding/#info","text":"def info ()","title":"info"},{"location":"embeddings/bert_embedding/#9595init9595","text":"def __init__ ( model_folder , layer_nums , trainable , task , sequence_length , processor , from_saved_model ) Args: task: model_folder: layer_nums: number of layers whose outputs will be concatenated into a single tensor, default 4 , output the last 4 hidden layers as the thesis suggested trainable: whether if the model is trainable, default False and set it to True for fine-tune this embedding layer during your training sequence_length: processor: from_saved_model","title":"__init__"},{"location":"embeddings/bert_embedding/#analyze95corpus","text":"def analyze_corpus ( x , y ) Prepare embedding layer and pre-processor for labeling task","title":"analyze_corpus"},{"location":"embeddings/bert_embedding/#args","text":"x : y :","title":"Args"},{"location":"embeddings/bert_embedding/#embed","text":"def embed ( sentence_list , debug ) batch embed sentences","title":"embed"},{"location":"embeddings/bert_embedding/#args_1","text":"sentence_list : Sentence list to embed debug : show debug log","title":"Args"},{"location":"embeddings/bert_embedding/#returns","text":"","title":"Returns"},{"location":"embeddings/bert_embedding/#process95x95dataset","text":"def process_x_dataset ( data , subset ) batch process feature data while training","title":"process_x_dataset"},{"location":"embeddings/bert_embedding/#args_2","text":"data : target dataset subset : subset index list","title":"Args"},{"location":"embeddings/bert_embedding/#returns_1","text":"","title":"Returns"},{"location":"embeddings/word_embedding/","text":"class WordEmbedding Pre-trained word2vec embedding info def info () __init__ def __init__ ( w2v_path , task , w2v_kwargs , sequence_length , processor , from_saved_model ) Args: task: w2v_path: word2vec file path w2v_kwargs: params pass to the load_word2vec_format() function of gensim.models.KeyedVectors - https://radimrehurek.com/gensim/models/keyedvectors.html#module-gensim.models.keyedvectors sequence_length: 'auto' , 'variable' or integer. When using 'auto' , use the 95% of corpus length as sequence length. When using 'variable' , model input shape will set to None, which can handle various length of input, it will use the length of max sequence in every batch for sequence length. If using an integer, let's say 50 , the input output sequence length will set to 50. processor analyze_corpus def analyze_corpus ( x , y ) Prepare embedding layer and pre-processor for labeling task Args x : y :","title":"word_embedding"},{"location":"embeddings/word_embedding/#class-wordembedding","text":"Pre-trained word2vec embedding","title":"class WordEmbedding"},{"location":"embeddings/word_embedding/#info","text":"def info ()","title":"info"},{"location":"embeddings/word_embedding/#9595init9595","text":"def __init__ ( w2v_path , task , w2v_kwargs , sequence_length , processor , from_saved_model ) Args: task: w2v_path: word2vec file path w2v_kwargs: params pass to the load_word2vec_format() function of gensim.models.KeyedVectors - https://radimrehurek.com/gensim/models/keyedvectors.html#module-gensim.models.keyedvectors sequence_length: 'auto' , 'variable' or integer. When using 'auto' , use the 95% of corpus length as sequence length. When using 'variable' , model input shape will set to None, which can handle various length of input, it will use the length of max sequence in every batch for sequence length. If using an integer, let's say 50 , the input output sequence length will set to 50. processor","title":"__init__"},{"location":"embeddings/word_embedding/#analyze95corpus","text":"def analyze_corpus ( x , y ) Prepare embedding layer and pre-processor for labeling task","title":"analyze_corpus"},{"location":"embeddings/word_embedding/#args","text":"x : y :","title":"Args"},{"location":"layers/att_wgt_avg_layer/","text":"class AttentionWeightedAverageLayer Computes a weighted average of the different channels across timesteps. Uses 1 parameter pr. channel to compute the attention value for a single timestep. __init__ def __init__ ( return_attention , ** kwargs ) build def build ( input_shape ) call def call ( x , mask ) get_output_shape_for def get_output_shape_for ( input_shape ) compute_output_shape def compute_output_shape ( input_shape ) compute_mask def compute_mask ( inputs , input_mask ) get_config def get_config ()","title":"Att wgt avg layer"},{"location":"layers/att_wgt_avg_layer/#class-attentionweightedaveragelayer","text":"Computes a weighted average of the different channels across timesteps. Uses 1 parameter pr. channel to compute the attention value for a single timestep.","title":"class AttentionWeightedAverageLayer"},{"location":"layers/att_wgt_avg_layer/#9595init9595","text":"def __init__ ( return_attention , ** kwargs )","title":"__init__"},{"location":"layers/att_wgt_avg_layer/#build","text":"def build ( input_shape )","title":"build"},{"location":"layers/att_wgt_avg_layer/#call","text":"def call ( x , mask )","title":"call"},{"location":"layers/att_wgt_avg_layer/#get95output95shape95for","text":"def get_output_shape_for ( input_shape )","title":"get_output_shape_for"},{"location":"layers/att_wgt_avg_layer/#compute95output95shape","text":"def compute_output_shape ( input_shape )","title":"compute_output_shape"},{"location":"layers/att_wgt_avg_layer/#compute95mask","text":"def compute_mask ( inputs , input_mask )","title":"compute_mask"},{"location":"layers/att_wgt_avg_layer/#get95config","text":"def get_config ()","title":"get_config"},{"location":"layers/crf/","text":"class CRF Conditional Random Field layer (tf.keras) CRF can be used as the last layer in a network (as a classifier). Input shape (features) must be equal to the number of classes the CRF can predict (a linear layer is recommended). Note: the loss and accuracy functions of networks using CRF must use the provided loss and accuracy functions (denoted as loss and viterbi_accuracy) as the classification of sequences are used with the layers internal weights. Args: output_dim (int): the number of labels to tag each temporal input. Input shape: nD tensor with shape (batch_size, sentence length, num_classes) . Output shape: nD tensor with shape: (batch_size, sentence length, num_classes) . __init__ def __init__ ( output_dim , mode , supports_masking , transitions , ** kwargs ) get_config def get_config () build def build ( input_shape ) call def call ( inputs , ** kwargs ) loss def loss ( y_true , y_pred ) compute_output_shape def compute_output_shape ( input_shape ) viterbi_accuracy def viterbi_accuracy ()","title":"Crf"},{"location":"layers/crf/#class-crf","text":"Conditional Random Field layer (tf.keras) CRF can be used as the last layer in a network (as a classifier). Input shape (features) must be equal to the number of classes the CRF can predict (a linear layer is recommended). Note: the loss and accuracy functions of networks using CRF must use the provided loss and accuracy functions (denoted as loss and viterbi_accuracy) as the classification of sequences are used with the layers internal weights. Args: output_dim (int): the number of labels to tag each temporal input. Input shape: nD tensor with shape (batch_size, sentence length, num_classes) . Output shape: nD tensor with shape: (batch_size, sentence length, num_classes) .","title":"class CRF"},{"location":"layers/crf/#9595init9595","text":"def __init__ ( output_dim , mode , supports_masking , transitions , ** kwargs )","title":"__init__"},{"location":"layers/crf/#get95config","text":"def get_config ()","title":"get_config"},{"location":"layers/crf/#build","text":"def build ( input_shape )","title":"build"},{"location":"layers/crf/#call","text":"def call ( inputs , ** kwargs )","title":"call"},{"location":"layers/crf/#loss","text":"def loss ( y_true , y_pred )","title":"loss"},{"location":"layers/crf/#compute95output95shape","text":"def compute_output_shape ( input_shape )","title":"compute_output_shape"},{"location":"layers/crf/#viterbi95accuracy","text":"def viterbi_accuracy ()","title":"viterbi_accuracy"},{"location":"layers/kmax_pool_layer/","text":"class KMaxPoolingLayer K-max pooling layer that extracts the k-highest activation from a sequence (2nd dimension). TensorFlow backend. # Arguments k: An int scale, indicate k max steps of features to pool. sorted: A bool, if output is sorted (default) or not. data_format: A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps) . # Input shape - If data_format='channels_last' : 3D tensor with shape: (batch_size, steps, features) - If data_format='channels_first' : 3D tensor with shape: (batch_size, features, steps) # Output shape 3D tensor with shape: (batch_size, top-k-steps, features) __init__ def __init__ ( k , sorted , data_format , ** kwargs ) compute_output_shape def compute_output_shape ( input_shape ) call def call ( inputs ) get_config def get_config ()","title":"Kmax pool layer"},{"location":"layers/kmax_pool_layer/#class-kmaxpoolinglayer","text":"K-max pooling layer that extracts the k-highest activation from a sequence (2nd dimension). TensorFlow backend. # Arguments k: An int scale, indicate k max steps of features to pool. sorted: A bool, if output is sorted (default) or not. data_format: A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps) . # Input shape - If data_format='channels_last' : 3D tensor with shape: (batch_size, steps, features) - If data_format='channels_first' : 3D tensor with shape: (batch_size, features, steps) # Output shape 3D tensor with shape: (batch_size, top-k-steps, features)","title":"class KMaxPoolingLayer"},{"location":"layers/kmax_pool_layer/#9595init9595","text":"def __init__ ( k , sorted , data_format , ** kwargs )","title":"__init__"},{"location":"layers/kmax_pool_layer/#compute95output95shape","text":"def compute_output_shape ( input_shape )","title":"compute_output_shape"},{"location":"layers/kmax_pool_layer/#call","text":"def call ( inputs )","title":"call"},{"location":"layers/kmax_pool_layer/#get95config","text":"def get_config ()","title":"get_config"},{"location":"layers/non_masking_layer/","text":"class NonMaskingLayer fix convolutional 1D can't receive masked input, detail: https://github.com/keras-team/keras/issues/4978 thanks for https://github.com/jacoxu __init__ def __init__ ( ** kwargs ) build def build ( input_shape ) compute_mask def compute_mask ( inputs , input_mask ) call def call ( x , mask )","title":"Non masking layer"},{"location":"layers/non_masking_layer/#class-nonmaskinglayer","text":"fix convolutional 1D can't receive masked input, detail: https://github.com/keras-team/keras/issues/4978 thanks for https://github.com/jacoxu","title":"class NonMaskingLayer"},{"location":"layers/non_masking_layer/#9595init9595","text":"def __init__ ( ** kwargs )","title":"__init__"},{"location":"layers/non_masking_layer/#build","text":"def build ( input_shape )","title":"build"},{"location":"layers/non_masking_layer/#compute95mask","text":"def compute_mask ( inputs , input_mask )","title":"compute_mask"},{"location":"layers/non_masking_layer/#call","text":"def call ( x , mask )","title":"call"},{"location":"processors/base_processor/","text":"class BaseProcessor Corpus Pre Processor class __init__ def __init__ ( ** kwargs ) info def info () analyze_corpus def analyze_corpus ( corpus , labels , force ) process_x_dataset def process_x_dataset ( data , max_len , subset ) process_y_dataset def process_y_dataset ( data , max_len , subset ) numerize_token_sequences def numerize_token_sequences ( sequences ) numerize_label_sequences def numerize_label_sequences ( sequences ) reverse_numerize_label_sequences def reverse_numerize_label_sequences ( sequence , ** kwargs ) __repr__ def __repr__ () __str__ def __str__ ()","title":"Base processor"},{"location":"processors/base_processor/#class-baseprocessor","text":"Corpus Pre Processor class","title":"class BaseProcessor"},{"location":"processors/base_processor/#9595init9595","text":"def __init__ ( ** kwargs )","title":"__init__"},{"location":"processors/base_processor/#info","text":"def info ()","title":"info"},{"location":"processors/base_processor/#analyze95corpus","text":"def analyze_corpus ( corpus , labels , force )","title":"analyze_corpus"},{"location":"processors/base_processor/#process95x95dataset","text":"def process_x_dataset ( data , max_len , subset )","title":"process_x_dataset"},{"location":"processors/base_processor/#process95y95dataset","text":"def process_y_dataset ( data , max_len , subset )","title":"process_y_dataset"},{"location":"processors/base_processor/#numerize95token95sequences","text":"def numerize_token_sequences ( sequences )","title":"numerize_token_sequences"},{"location":"processors/base_processor/#numerize95label95sequences","text":"def numerize_label_sequences ( sequences )","title":"numerize_label_sequences"},{"location":"processors/base_processor/#reverse95numerize95label95sequences","text":"def reverse_numerize_label_sequences ( sequence , ** kwargs )","title":"reverse_numerize_label_sequences"},{"location":"processors/base_processor/#9595repr9595","text":"def __repr__ ()","title":"__repr__"},{"location":"processors/base_processor/#9595str9595","text":"def __str__ ()","title":"__str__"},{"location":"processors/classification_processor/","text":"class ClassificationProcessor Corpus Pre Processor class __init__ def __init__ ( multi_label , ** kwargs ) info def info () process_y_dataset def process_y_dataset ( data , max_len , subset ) numerize_token_sequences def numerize_token_sequences ( sequences ) numerize_label_sequences def numerize_label_sequences ( sequences ) Convert label sequence to label-index sequence ['O', 'O', 'B-ORG'] -> [0, 0, 2] Args sequences : label sequence, list of str Returns reverse_numerize_label_sequences def reverse_numerize_label_sequences ( sequences , ** kwargs )","title":"Classification processor"},{"location":"processors/classification_processor/#class-classificationprocessor","text":"Corpus Pre Processor class","title":"class ClassificationProcessor"},{"location":"processors/classification_processor/#9595init9595","text":"def __init__ ( multi_label , ** kwargs )","title":"__init__"},{"location":"processors/classification_processor/#info","text":"def info ()","title":"info"},{"location":"processors/classification_processor/#process95y95dataset","text":"def process_y_dataset ( data , max_len , subset )","title":"process_y_dataset"},{"location":"processors/classification_processor/#numerize95token95sequences","text":"def numerize_token_sequences ( sequences )","title":"numerize_token_sequences"},{"location":"processors/classification_processor/#numerize95label95sequences","text":"def numerize_label_sequences ( sequences ) Convert label sequence to label-index sequence ['O', 'O', 'B-ORG'] -> [0, 0, 2]","title":"numerize_label_sequences"},{"location":"processors/classification_processor/#args","text":"sequences : label sequence, list of str","title":"Args"},{"location":"processors/classification_processor/#returns","text":"","title":"Returns"},{"location":"processors/classification_processor/#reverse95numerize95label95sequences","text":"def reverse_numerize_label_sequences ( sequences , ** kwargs )","title":"reverse_numerize_label_sequences"},{"location":"processors/labeling_processor/","text":"class LabelingProcessor Corpus Pre Processor class info def info () process_y_dataset def process_y_dataset ( data , max_len , subset ) numerize_token_sequences def numerize_token_sequences ( sequences ) numerize_label_sequences def numerize_label_sequences ( sequences ) reverse_numerize_label_sequences def reverse_numerize_label_sequences ( sequences , lengths )","title":"Labeling processor"},{"location":"processors/labeling_processor/#class-labelingprocessor","text":"Corpus Pre Processor class","title":"class LabelingProcessor"},{"location":"processors/labeling_processor/#info","text":"def info ()","title":"info"},{"location":"processors/labeling_processor/#process95y95dataset","text":"def process_y_dataset ( data , max_len , subset )","title":"process_y_dataset"},{"location":"processors/labeling_processor/#numerize95token95sequences","text":"def numerize_token_sequences ( sequences )","title":"numerize_token_sequences"},{"location":"processors/labeling_processor/#numerize95label95sequences","text":"def numerize_label_sequences ( sequences )","title":"numerize_label_sequences"},{"location":"processors/labeling_processor/#reverse95numerize95label95sequences","text":"def reverse_numerize_label_sequences ( sequences , lengths )","title":"reverse_numerize_label_sequences"},{"location":"tasks/base_model/","text":"class BaseModel Base Sequence Labeling Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) info def info () task def task () token2idx def token2idx () label2idx def label2idx () pre_processor def pre_processor () processor def processor () __init__ def __init__ ( embedding , hyper_parameters ) Args: embedding: model embedding hyper_parameters: a dict of hyper_parameters. Examples: You could change customize hyper_parameters like this:: get default hyper_parameters hyper_parameters = BLSTMModel.get_default_hyper_parameters() # change lstm hidden unit to 12 hyper_parameters['layer_blstm']['units'] = 12 # init new model with customized hyper_parameters labeling_model = BLSTMModel(hyper_parameters=hyper_parameters) labeling_model.fit(x, y) build_model def build_model ( x_train , y_train , x_validate , y_validate ) Build model with corpus Args x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data build_multi_gpu_model def build_multi_gpu_model ( gpus , x_train , y_train , cpu_merge , cpu_relocation , x_validate , y_validate ) Build multi-GPU model with corpus Args gpus : Integer >= 2, number of on GPUs on which to create model replicas. cpu_merge : A boolean value to identify whether to force merging model weights under the scope of the CPU or not. cpu_relocation : A boolean value to identify whether to create the model's weights under the scope of the CPU. If the model is not defined under any preceding device scope, you can still rescue it by activating this option. x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data build_tpu_model def build_tpu_model ( strategy , x_train , y_train , x_validate , y_validate ) Build TPU model with corpus Args strategy : TPUDistributionStrategy . The strategy to use for replicating model across multiple TPU cores. x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data get_data_generator def get_data_generator ( x_data , y_data , batch_size , shuffle ) data generator for fit_generator Args x_data : Array of feature data (if the model has a single input), or tuple of feature data array (if the model has multiple inputs) y_data : Array of label data batch_size : Number of samples per gradient update, default to 64. shuffle : Returns fit def fit ( x_train , y_train , x_validate , y_validate , batch_size , epochs , callbacks , fit_kwargs , shuffle ) Trains the model for a given number of epochs with fit_generator (iterations on a dataset). Args x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data batch_size : Number of samples per gradient update, default to 64. epochs : Integer. Number of epochs to train the model. default 5. callbacks : fit_kwargs : fit_kwargs shuffle : fit_without_generator def fit_without_generator ( x_train , y_train , x_validate , y_validate , batch_size , epochs , callbacks , fit_kwargs ) Trains the model for a given number of epochs (iterations on a dataset). Args x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data batch_size : Number of samples per gradient update, default to 64. epochs : Integer. Number of epochs to train the model. default 5. callbacks : fit_kwargs : fit_kwargs compile_model def compile_model ( ** kwargs ) Configures the model for training. Using compile() function of tf.keras.Model https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#compile Args **kwargs : arguments passed to compile() function of tf.keras.Model ults : - loss : categorical_crossentropy - optimizer : adam - metrics : ['accuracy'] predict def predict ( x_data , batch_size , debug_info , predict_kwargs ) Generates output predictions for the input samples. Computation is done in batches. Args x_data : The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs). batch_size : Integer. If unspecified, it will default to 32. debug_info : Bool, Should print out the logging info. predict_kwargs : arguments passed to predict() function of tf.keras.Model Returns evaluate def evaluate ( x_data , y_data , batch_size , digits , debug_info ) Evaluate model Args: x_data: y_data: batch_size: digits: debug_info build_model_arc def build_model_arc () save def save ( model_path ) Save model Args: model_path","title":"Base model"},{"location":"tasks/base_model/#class-basemodel","text":"Base Sequence Labeling Model","title":"class BaseModel"},{"location":"tasks/base_model/#get95default95hyper95parameters","text":"def get_default_hyper_parameters ( cls )","title":"get_default_hyper_parameters"},{"location":"tasks/base_model/#info","text":"def info ()","title":"info"},{"location":"tasks/base_model/#task","text":"def task ()","title":"task"},{"location":"tasks/base_model/#token2idx","text":"def token2idx ()","title":"token2idx"},{"location":"tasks/base_model/#label2idx","text":"def label2idx ()","title":"label2idx"},{"location":"tasks/base_model/#pre95processor","text":"def pre_processor ()","title":"pre_processor"},{"location":"tasks/base_model/#processor","text":"def processor ()","title":"processor"},{"location":"tasks/base_model/#9595init9595","text":"def __init__ ( embedding , hyper_parameters ) Args: embedding: model embedding hyper_parameters: a dict of hyper_parameters. Examples: You could change customize hyper_parameters like this::","title":"__init__"},{"location":"tasks/base_model/#get-default-hyper_parameters-hyper_parameters-blstmmodelget_default_hyper_parameters-change-lstm-hidden-unit-to-12-hyper_parameterslayer_blstmunits-12-init-new-model-with-customized-hyper_parameters-labeling_model-blstmmodelhyper_parametershyper_parameters-labeling_modelfitx-y","text":"","title":"get default hyper_parameters hyper_parameters = BLSTMModel.get_default_hyper_parameters() # change lstm hidden unit to 12 hyper_parameters['layer_blstm']['units'] = 12 # init new model with customized hyper_parameters labeling_model = BLSTMModel(hyper_parameters=hyper_parameters) labeling_model.fit(x, y)"},{"location":"tasks/base_model/#build95model","text":"def build_model ( x_train , y_train , x_validate , y_validate ) Build model with corpus","title":"build_model"},{"location":"tasks/base_model/#args","text":"x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data","title":"Args"},{"location":"tasks/base_model/#build95multi95gpu95model","text":"def build_multi_gpu_model ( gpus , x_train , y_train , cpu_merge , cpu_relocation , x_validate , y_validate ) Build multi-GPU model with corpus","title":"build_multi_gpu_model"},{"location":"tasks/base_model/#args_1","text":"gpus : Integer >= 2, number of on GPUs on which to create model replicas. cpu_merge : A boolean value to identify whether to force merging model weights under the scope of the CPU or not. cpu_relocation : A boolean value to identify whether to create the model's weights under the scope of the CPU. If the model is not defined under any preceding device scope, you can still rescue it by activating this option. x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data","title":"Args"},{"location":"tasks/base_model/#build95tpu95model","text":"def build_tpu_model ( strategy , x_train , y_train , x_validate , y_validate ) Build TPU model with corpus","title":"build_tpu_model"},{"location":"tasks/base_model/#args_2","text":"strategy : TPUDistributionStrategy . The strategy to use for replicating model across multiple TPU cores. x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data","title":"Args"},{"location":"tasks/base_model/#get95data95generator","text":"def get_data_generator ( x_data , y_data , batch_size , shuffle ) data generator for fit_generator","title":"get_data_generator"},{"location":"tasks/base_model/#args_3","text":"x_data : Array of feature data (if the model has a single input), or tuple of feature data array (if the model has multiple inputs) y_data : Array of label data batch_size : Number of samples per gradient update, default to 64. shuffle :","title":"Args"},{"location":"tasks/base_model/#returns","text":"","title":"Returns"},{"location":"tasks/base_model/#fit","text":"def fit ( x_train , y_train , x_validate , y_validate , batch_size , epochs , callbacks , fit_kwargs , shuffle ) Trains the model for a given number of epochs with fit_generator (iterations on a dataset).","title":"fit"},{"location":"tasks/base_model/#args_4","text":"x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data batch_size : Number of samples per gradient update, default to 64. epochs : Integer. Number of epochs to train the model. default 5. callbacks : fit_kwargs : fit_kwargs shuffle :","title":"Args"},{"location":"tasks/base_model/#fit95without95generator","text":"def fit_without_generator ( x_train , y_train , x_validate , y_validate , batch_size , epochs , callbacks , fit_kwargs ) Trains the model for a given number of epochs (iterations on a dataset).","title":"fit_without_generator"},{"location":"tasks/base_model/#args_5","text":"x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data batch_size : Number of samples per gradient update, default to 64. epochs : Integer. Number of epochs to train the model. default 5. callbacks : fit_kwargs : fit_kwargs","title":"Args"},{"location":"tasks/base_model/#compile95model","text":"def compile_model ( ** kwargs ) Configures the model for training. Using compile() function of tf.keras.Model https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#compile","title":"compile_model"},{"location":"tasks/base_model/#args_6","text":"**kwargs : arguments passed to compile() function of tf.keras.Model ults : - loss : categorical_crossentropy - optimizer : adam - metrics : ['accuracy']","title":"Args"},{"location":"tasks/base_model/#predict","text":"def predict ( x_data , batch_size , debug_info , predict_kwargs ) Generates output predictions for the input samples. Computation is done in batches.","title":"predict"},{"location":"tasks/base_model/#args_7","text":"x_data : The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs). batch_size : Integer. If unspecified, it will default to 32. debug_info : Bool, Should print out the logging info. predict_kwargs : arguments passed to predict() function of tf.keras.Model","title":"Args"},{"location":"tasks/base_model/#returns_1","text":"","title":"Returns"},{"location":"tasks/base_model/#evaluate","text":"def evaluate ( x_data , y_data , batch_size , digits , debug_info ) Evaluate model Args: x_data: y_data: batch_size: digits: debug_info","title":"evaluate"},{"location":"tasks/base_model/#build95model95arc","text":"def build_model_arc ()","title":"build_model_arc"},{"location":"tasks/base_model/#save","text":"def save ( model_path ) Save model Args: model_path","title":"save"},{"location":"tasks/classification/base_model/","text":"class BaseClassificationModel __init__ def __init__ ( embedding , hyper_parameters ) get_default_hyper_parameters def get_default_hyper_parameters ( cls ) build_model_arc def build_model_arc () compile_model def compile_model ( ** kwargs ) predict def predict ( x_data , batch_size , multi_label_threshold , debug_info , predict_kwargs ) Generates output predictions for the input samples. Computation is done in batches. Args x_data : The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs). batch_size : Integer. If unspecified, it will default to 32. multi_label_threshold : debug_info : Bool, Should print out the logging info. predict_kwargs : arguments passed to predict() function of tf.keras.Model Returns predict_top_k_class def predict_top_k_class ( x_data , top_k , batch_size , debug_info , predict_kwargs ) Generates output predictions with confidence for the input samples. Computation is done in batches. Args x_data : The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs). top_k : int batch_size : Integer. If unspecified, it will default to 32. debug_info : Bool, Should print out the logging info. predict_kwargs : arguments passed to predict() function of tf.keras.Model Returns single-label classification : [ { \"label\" multi-label classification : [ { \"candidates\" evaluate def evaluate ( x_data , y_data , batch_size , digits , output_dict , debug_info )","title":"Base model"},{"location":"tasks/classification/base_model/#class-baseclassificationmodel","text":"","title":"class BaseClassificationModel"},{"location":"tasks/classification/base_model/#9595init9595","text":"def __init__ ( embedding , hyper_parameters )","title":"__init__"},{"location":"tasks/classification/base_model/#get95default95hyper95parameters","text":"def get_default_hyper_parameters ( cls )","title":"get_default_hyper_parameters"},{"location":"tasks/classification/base_model/#build95model95arc","text":"def build_model_arc ()","title":"build_model_arc"},{"location":"tasks/classification/base_model/#compile95model","text":"def compile_model ( ** kwargs )","title":"compile_model"},{"location":"tasks/classification/base_model/#predict","text":"def predict ( x_data , batch_size , multi_label_threshold , debug_info , predict_kwargs ) Generates output predictions for the input samples. Computation is done in batches.","title":"predict"},{"location":"tasks/classification/base_model/#args","text":"x_data : The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs). batch_size : Integer. If unspecified, it will default to 32. multi_label_threshold : debug_info : Bool, Should print out the logging info. predict_kwargs : arguments passed to predict() function of tf.keras.Model","title":"Args"},{"location":"tasks/classification/base_model/#returns","text":"","title":"Returns"},{"location":"tasks/classification/base_model/#predict95top95k95class","text":"def predict_top_k_class ( x_data , top_k , batch_size , debug_info , predict_kwargs ) Generates output predictions with confidence for the input samples. Computation is done in batches.","title":"predict_top_k_class"},{"location":"tasks/classification/base_model/#args_1","text":"x_data : The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs). top_k : int batch_size : Integer. If unspecified, it will default to 32. debug_info : Bool, Should print out the logging info. predict_kwargs : arguments passed to predict() function of tf.keras.Model","title":"Args"},{"location":"tasks/classification/base_model/#returns_1","text":"single-label classification : [ { \"label\" multi-label classification : [ { \"candidates\"","title":"Returns"},{"location":"tasks/classification/base_model/#evaluate","text":"def evaluate ( x_data , y_data , batch_size , digits , output_dict , debug_info )","title":"evaluate"},{"location":"tasks/classification/dpcnn_model/","text":"class DPCNN_Model This implementation of DPCNN requires a clear declared sequence length. So sequences input in should be padded or cut to a given length in advance. get_default_hyper_parameters def get_default_hyper_parameters ( cls ) downsample def downsample ( inputs , pool_type , sorted , stage ) conv_block def conv_block ( inputs , filters , kernel_size , activation , shortcut ) resnet_block def resnet_block ( inputs , filters , kernel_size , activation , shortcut , pool_type , sorted , stage ) build_model_arc def build_model_arc ()","title":"Dpcnn model"},{"location":"tasks/classification/dpcnn_model/#class-dpcnn_model","text":"This implementation of DPCNN requires a clear declared sequence length. So sequences input in should be padded or cut to a given length in advance.","title":"class DPCNN_Model"},{"location":"tasks/classification/dpcnn_model/#get95default95hyper95parameters","text":"def get_default_hyper_parameters ( cls )","title":"get_default_hyper_parameters"},{"location":"tasks/classification/dpcnn_model/#downsample","text":"def downsample ( inputs , pool_type , sorted , stage )","title":"downsample"},{"location":"tasks/classification/dpcnn_model/#conv95block","text":"def conv_block ( inputs , filters , kernel_size , activation , shortcut )","title":"conv_block"},{"location":"tasks/classification/dpcnn_model/#resnet95block","text":"def resnet_block ( inputs , filters , kernel_size , activation , shortcut , pool_type , sorted , stage )","title":"resnet_block"},{"location":"tasks/classification/dpcnn_model/#build95model95arc","text":"def build_model_arc ()","title":"build_model_arc"},{"location":"tasks/classification/models/","text":"class BiLSTM_Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) build_model_arc def build_model_arc () class BiGRU_Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) build_model_arc def build_model_arc () class CNN_Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) build_model_arc def build_model_arc () class CNN_LSTM_Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) build_model_arc def build_model_arc () class CNN_GRU_Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) build_model_arc def build_model_arc () class AVCNN_Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) build_model_arc def build_model_arc () class KMax_CNN_Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) build_model_arc def build_model_arc () class R_CNN_Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) build_model_arc def build_model_arc () class AVRNN_Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) build_model_arc def build_model_arc () class Dropout_BiGRU_Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) build_model_arc def build_model_arc () class Dropout_AVRNN_Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) build_model_arc def build_model_arc ()","title":"Models"},{"location":"tasks/classification/models/#class-bilstm_model","text":"","title":"class BiLSTM_Model"},{"location":"tasks/classification/models/#get95default95hyper95parameters","text":"def get_default_hyper_parameters ( cls )","title":"get_default_hyper_parameters"},{"location":"tasks/classification/models/#build95model95arc","text":"def build_model_arc ()","title":"build_model_arc"},{"location":"tasks/classification/models/#class-bigru_model","text":"","title":"class BiGRU_Model"},{"location":"tasks/classification/models/#get95default95hyper95parameters_1","text":"def get_default_hyper_parameters ( cls )","title":"get_default_hyper_parameters"},{"location":"tasks/classification/models/#build95model95arc_1","text":"def build_model_arc ()","title":"build_model_arc"},{"location":"tasks/classification/models/#class-cnn_model","text":"","title":"class CNN_Model"},{"location":"tasks/classification/models/#get95default95hyper95parameters_2","text":"def get_default_hyper_parameters ( cls )","title":"get_default_hyper_parameters"},{"location":"tasks/classification/models/#build95model95arc_2","text":"def build_model_arc ()","title":"build_model_arc"},{"location":"tasks/classification/models/#class-cnn_lstm_model","text":"","title":"class CNN_LSTM_Model"},{"location":"tasks/classification/models/#get95default95hyper95parameters_3","text":"def get_default_hyper_parameters ( cls )","title":"get_default_hyper_parameters"},{"location":"tasks/classification/models/#build95model95arc_3","text":"def build_model_arc ()","title":"build_model_arc"},{"location":"tasks/classification/models/#class-cnn_gru_model","text":"","title":"class CNN_GRU_Model"},{"location":"tasks/classification/models/#get95default95hyper95parameters_4","text":"def get_default_hyper_parameters ( cls )","title":"get_default_hyper_parameters"},{"location":"tasks/classification/models/#build95model95arc_4","text":"def build_model_arc ()","title":"build_model_arc"},{"location":"tasks/classification/models/#class-avcnn_model","text":"","title":"class AVCNN_Model"},{"location":"tasks/classification/models/#get95default95hyper95parameters_5","text":"def get_default_hyper_parameters ( cls )","title":"get_default_hyper_parameters"},{"location":"tasks/classification/models/#build95model95arc_5","text":"def build_model_arc ()","title":"build_model_arc"},{"location":"tasks/classification/models/#class-kmax_cnn_model","text":"","title":"class KMax_CNN_Model"},{"location":"tasks/classification/models/#get95default95hyper95parameters_6","text":"def get_default_hyper_parameters ( cls )","title":"get_default_hyper_parameters"},{"location":"tasks/classification/models/#build95model95arc_6","text":"def build_model_arc ()","title":"build_model_arc"},{"location":"tasks/classification/models/#class-r_cnn_model","text":"","title":"class R_CNN_Model"},{"location":"tasks/classification/models/#get95default95hyper95parameters_7","text":"def get_default_hyper_parameters ( cls )","title":"get_default_hyper_parameters"},{"location":"tasks/classification/models/#build95model95arc_7","text":"def build_model_arc ()","title":"build_model_arc"},{"location":"tasks/classification/models/#class-avrnn_model","text":"","title":"class AVRNN_Model"},{"location":"tasks/classification/models/#get95default95hyper95parameters_8","text":"def get_default_hyper_parameters ( cls )","title":"get_default_hyper_parameters"},{"location":"tasks/classification/models/#build95model95arc_8","text":"def build_model_arc ()","title":"build_model_arc"},{"location":"tasks/classification/models/#class-dropout_bigru_model","text":"","title":"class Dropout_BiGRU_Model"},{"location":"tasks/classification/models/#get95default95hyper95parameters_9","text":"def get_default_hyper_parameters ( cls )","title":"get_default_hyper_parameters"},{"location":"tasks/classification/models/#build95model95arc_9","text":"def build_model_arc ()","title":"build_model_arc"},{"location":"tasks/classification/models/#class-dropout_avrnn_model","text":"","title":"class Dropout_AVRNN_Model"},{"location":"tasks/classification/models/#get95default95hyper95parameters_10","text":"def get_default_hyper_parameters ( cls )","title":"get_default_hyper_parameters"},{"location":"tasks/classification/models/#build95model95arc_10","text":"def build_model_arc ()","title":"build_model_arc"},{"location":"tasks/labeling/base_model/","text":"class BaseLabelingModel Base Sequence Labeling Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) predict_entities def predict_entities ( x_data , batch_size , join_chunk , debug_info , predict_kwargs ) Gets entities from sequence. Args x_data : The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs). batch_size : Integer. If unspecified, it will default to 32. join_chunk : str or False, debug_info : Bool, Should print out the logging info. predict_kwargs : arguments passed to predict() function of tf.keras.Model Returns list : list of entity. evaluate def evaluate ( x_data , y_data , batch_size , digits , debug_info ) Build a text report showing the main classification metrics. Args x_data : y_data : batch_size : digits : debug_info : build_model_arc def build_model_arc ()","title":"Base model"},{"location":"tasks/labeling/base_model/#class-baselabelingmodel","text":"Base Sequence Labeling Model","title":"class BaseLabelingModel"},{"location":"tasks/labeling/base_model/#get95default95hyper95parameters","text":"def get_default_hyper_parameters ( cls )","title":"get_default_hyper_parameters"},{"location":"tasks/labeling/base_model/#predict95entities","text":"def predict_entities ( x_data , batch_size , join_chunk , debug_info , predict_kwargs ) Gets entities from sequence.","title":"predict_entities"},{"location":"tasks/labeling/base_model/#args","text":"x_data : The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs). batch_size : Integer. If unspecified, it will default to 32. join_chunk : str or False, debug_info : Bool, Should print out the logging info. predict_kwargs : arguments passed to predict() function of tf.keras.Model","title":"Args"},{"location":"tasks/labeling/base_model/#returns","text":"list : list of entity.","title":"Returns"},{"location":"tasks/labeling/base_model/#evaluate","text":"def evaluate ( x_data , y_data , batch_size , digits , debug_info ) Build a text report showing the main classification metrics.","title":"evaluate"},{"location":"tasks/labeling/base_model/#args_1","text":"x_data : y_data : batch_size : digits : debug_info :","title":"Args"},{"location":"tasks/labeling/base_model/#build95model95arc","text":"def build_model_arc ()","title":"build_model_arc"},{"location":"tasks/labeling/experimental/","text":"class BLSTMAttentionModel Bidirectional LSTM Self Attention Sequence Labeling Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) Get hyper parameters of model Returns: hyper parameters dict build_model_arc def build_model_arc () build model architectural","title":"Experimental"},{"location":"tasks/labeling/experimental/#class-blstmattentionmodel","text":"Bidirectional LSTM Self Attention Sequence Labeling Model","title":"class BLSTMAttentionModel"},{"location":"tasks/labeling/experimental/#get95default95hyper95parameters","text":"def get_default_hyper_parameters ( cls ) Get hyper parameters of model Returns: hyper parameters dict","title":"get_default_hyper_parameters"},{"location":"tasks/labeling/experimental/#build95model95arc","text":"def build_model_arc () build model architectural","title":"build_model_arc"},{"location":"tasks/labeling/models/","text":"class BiLSTM_Model Bidirectional LSTM Sequence Labeling Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) Get hyper parameters of model Returns: hyper parameters dict build_model_arc def build_model_arc () build model architectural class BiLSTM_CRF_Model Bidirectional LSTM CRF Sequence Labeling Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) Get hyper parameters of model Returns: hyper parameters dict build_model_arc def build_model_arc () build model architectural compile_model def compile_model ( ** kwargs ) class BiGRU_Model Bidirectional GRU Sequence Labeling Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) Get hyper parameters of model Returns: hyper parameters dict build_model_arc def build_model_arc () build model architectural class BiGRU_CRF_Model Bidirectional GRU CRF Sequence Labeling Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) Get hyper parameters of model Returns: hyper parameters dict build_model_arc def build_model_arc () build model architectural compile_model def compile_model ( ** kwargs ) class CNN_LSTM_Model CNN LSTM Sequence Labeling Model get_default_hyper_parameters def get_default_hyper_parameters ( cls ) Get hyper parameters of model Returns: hyper parameters dict build_model_arc def build_model_arc () build model architectural","title":"Models"},{"location":"tasks/labeling/models/#class-bilstm_model","text":"Bidirectional LSTM Sequence Labeling Model","title":"class BiLSTM_Model"},{"location":"tasks/labeling/models/#get95default95hyper95parameters","text":"def get_default_hyper_parameters ( cls ) Get hyper parameters of model Returns: hyper parameters dict","title":"get_default_hyper_parameters"},{"location":"tasks/labeling/models/#build95model95arc","text":"def build_model_arc () build model architectural","title":"build_model_arc"},{"location":"tasks/labeling/models/#class-bilstm_crf_model","text":"Bidirectional LSTM CRF Sequence Labeling Model","title":"class BiLSTM_CRF_Model"},{"location":"tasks/labeling/models/#get95default95hyper95parameters_1","text":"def get_default_hyper_parameters ( cls ) Get hyper parameters of model Returns: hyper parameters dict","title":"get_default_hyper_parameters"},{"location":"tasks/labeling/models/#build95model95arc_1","text":"def build_model_arc () build model architectural","title":"build_model_arc"},{"location":"tasks/labeling/models/#compile95model","text":"def compile_model ( ** kwargs )","title":"compile_model"},{"location":"tasks/labeling/models/#class-bigru_model","text":"Bidirectional GRU Sequence Labeling Model","title":"class BiGRU_Model"},{"location":"tasks/labeling/models/#get95default95hyper95parameters_2","text":"def get_default_hyper_parameters ( cls ) Get hyper parameters of model Returns: hyper parameters dict","title":"get_default_hyper_parameters"},{"location":"tasks/labeling/models/#build95model95arc_2","text":"def build_model_arc () build model architectural","title":"build_model_arc"},{"location":"tasks/labeling/models/#class-bigru_crf_model","text":"Bidirectional GRU CRF Sequence Labeling Model","title":"class BiGRU_CRF_Model"},{"location":"tasks/labeling/models/#get95default95hyper95parameters_3","text":"def get_default_hyper_parameters ( cls ) Get hyper parameters of model Returns: hyper parameters dict","title":"get_default_hyper_parameters"},{"location":"tasks/labeling/models/#build95model95arc_3","text":"def build_model_arc () build model architectural","title":"build_model_arc"},{"location":"tasks/labeling/models/#compile95model_1","text":"def compile_model ( ** kwargs )","title":"compile_model"},{"location":"tasks/labeling/models/#class-cnn_lstm_model","text":"CNN LSTM Sequence Labeling Model","title":"class CNN_LSTM_Model"},{"location":"tasks/labeling/models/#get95default95hyper95parameters_4","text":"def get_default_hyper_parameters ( cls ) Get hyper parameters of model Returns: hyper parameters dict","title":"get_default_hyper_parameters"},{"location":"tasks/labeling/models/#build95model95arc_4","text":"def build_model_arc () build model architectural","title":"build_model_arc"}]}