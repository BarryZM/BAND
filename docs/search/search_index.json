{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BAND\uff1aBERT Application aNd Deployment Simple and efficient BERT model development and deployment, \u7b80\u5355\u9ad8\u6548\u7684 BERT \u6a21\u578b\u5f00\u53d1\u548c\u90e8\u7f72 BAND BAND\uff1aBERT Application aNd Deployment \u63a2\u7d22\u672c\u9879\u76ee\u7684\u6587\u6863 \u00bb \u67e5\u770bDemo \u00b7 \u62a5\u544aBug \u00b7 \u63d0\u51fa\u65b0\u7279\u6027 \u00b7 \u95ee\u9898\u4ea4\u6d41 \u76ee\u5f55 \u4e0a\u624b\u6307\u5357 \u5f00\u53d1\u524d\u7684\u914d\u7f6e\u8981\u6c42 \u5b89\u88c5\u6b65\u9aa4 \u6587\u4ef6\u76ee\u5f55\u8bf4\u660e \u5f00\u53d1\u7684\u67b6\u6784 \u90e8\u7f72 \u4f7f\u7528\u5230\u7684\u6846\u67b6 \u8d21\u732e\u8005 \u5982\u4f55\u53c2\u4e0e\u5f00\u6e90\u9879\u76ee \u7248\u672c\u63a7\u5236 \u4f5c\u8005 \u9e23\u8c22 \u4e0a\u624b\u6307\u5357 \u5f00\u53d1\u524d\u7684\u914d\u7f6e\u8981\u6c42 xxxxx x.x.x xxxxx x.x.x \u5b89\u88c5\u6b65\u9aa4 Clone the repo git clone https://github.com/SunYanCN/BAND.git \u6587\u4ef6\u76ee\u5f55\u8bf4\u660e filetree \u251c\u2500\u2500 ARCHITECTURE.md \u251c\u2500\u2500 LICENSE.txt \u251c\u2500\u2500 README.md \u251c\u2500\u2500 /account/ \u251c\u2500\u2500 /bbs/ \u251c\u2500\u2500 /docs/ \u2502 \u251c\u2500\u2500 /rules/ \u2502 \u2502 \u251c\u2500\u2500 backend.txt \u2502 \u2502 \u2514\u2500\u2500 frontend.txt \u251c\u2500\u2500 manage.py \u251c\u2500\u2500 /oa/ \u251c\u2500\u2500 /static/ \u251c\u2500\u2500 /templates/ \u251c\u2500\u2500 useless.md \u2514\u2500\u2500 /util/ \u90e8\u7f72 \u6682\u65e0 \u4f7f\u7528\u5230\u7684\u6846\u67b6 TensorFlow simple-tensorflow-serving \u4f5c\u8005 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u8054\u7cfb\u6211\uff1a - Email : sunyanhust@163.com - NLP\u6280\u672fQQ\u4ea4\u6d41\u7fa4 \uff1a859886087 \u60a8\u4e5f\u53ef\u4ee5\u5728\u8d21\u732e\u8005\u540d\u5355\u4e2d\u53c2\u770b\u6240\u6709\u53c2\u4e0e\u8be5\u9879\u76ee\u7684\u5f00\u53d1\u8005\u3002 \u8d21\u732e\u8005 \u8bf7\u9605\u8bfb CONTRIBUTING.md \u67e5\u9605\u4e3a\u8be5\u9879\u76ee\u505a\u51fa\u8d21\u732e\u7684\u5f00\u53d1\u8005\u3002 \u5982\u4f55\u53c2\u4e0e\u5f00\u6e90\u9879\u76ee \u8d21\u732e\u4f7f\u5f00\u6e90\u793e\u533a\u6210\u4e3a\u4e00\u4e2a\u5b66\u4e60\u3001\u6fc0\u52b1\u548c\u521b\u9020\u7684\u7edd\u4f73\u573a\u6240\u3002\u4f60\u6240\u4f5c\u7684\u4efb\u4f55\u8d21\u732e\u90fd\u662f \u975e\u5e38\u611f\u8c22 \u7684\u3002 Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request \u7248\u6743\u8bf4\u660e \u8be5\u9879\u76ee\u7b7e\u7f72\u4e86MIT \u6388\u6743\u8bb8\u53ef\uff0c\u8be6\u60c5\u8bf7\u53c2\u9605 LICENSE \u7248\u672c\u63a7\u5236 \u8be5\u9879\u76ee\u4f7f\u7528Git\u8fdb\u884c\u7248\u672c\u7ba1\u7406\u3002\u60a8\u53ef\u4ee5\u5728repository\u53c2\u770b\u5f53\u524d\u53ef\u7528\u7248\u672c\u3002 \u9e23\u8c22 Free Logo Design","title":"Home"},{"location":"#bandbert-application-and-deployment","text":"Simple and efficient BERT model development and deployment, \u7b80\u5355\u9ad8\u6548\u7684 BERT \u6a21\u578b\u5f00\u53d1\u548c\u90e8\u7f72","title":"BAND\uff1aBERT Application aNd Deployment"},{"location":"#_1","text":"\u4e0a\u624b\u6307\u5357 \u5f00\u53d1\u524d\u7684\u914d\u7f6e\u8981\u6c42 \u5b89\u88c5\u6b65\u9aa4 \u6587\u4ef6\u76ee\u5f55\u8bf4\u660e \u5f00\u53d1\u7684\u67b6\u6784 \u90e8\u7f72 \u4f7f\u7528\u5230\u7684\u6846\u67b6 \u8d21\u732e\u8005 \u5982\u4f55\u53c2\u4e0e\u5f00\u6e90\u9879\u76ee \u7248\u672c\u63a7\u5236 \u4f5c\u8005 \u9e23\u8c22","title":"\u76ee\u5f55"},{"location":"#_2","text":"","title":"\u4e0a\u624b\u6307\u5357"},{"location":"#_3","text":"xxxxx x.x.x xxxxx x.x.x","title":"\u5f00\u53d1\u524d\u7684\u914d\u7f6e\u8981\u6c42"},{"location":"#_4","text":"Clone the repo git clone https://github.com/SunYanCN/BAND.git","title":"\u5b89\u88c5\u6b65\u9aa4"},{"location":"#_5","text":"filetree \u251c\u2500\u2500 ARCHITECTURE.md \u251c\u2500\u2500 LICENSE.txt \u251c\u2500\u2500 README.md \u251c\u2500\u2500 /account/ \u251c\u2500\u2500 /bbs/ \u251c\u2500\u2500 /docs/ \u2502 \u251c\u2500\u2500 /rules/ \u2502 \u2502 \u251c\u2500\u2500 backend.txt \u2502 \u2502 \u2514\u2500\u2500 frontend.txt \u251c\u2500\u2500 manage.py \u251c\u2500\u2500 /oa/ \u251c\u2500\u2500 /static/ \u251c\u2500\u2500 /templates/ \u251c\u2500\u2500 useless.md \u2514\u2500\u2500 /util/","title":"\u6587\u4ef6\u76ee\u5f55\u8bf4\u660e"},{"location":"#_6","text":"\u6682\u65e0","title":"\u90e8\u7f72"},{"location":"#_7","text":"TensorFlow simple-tensorflow-serving","title":"\u4f7f\u7528\u5230\u7684\u6846\u67b6"},{"location":"#_8","text":"\u60a8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u8054\u7cfb\u6211\uff1a - Email : sunyanhust@163.com - NLP\u6280\u672fQQ\u4ea4\u6d41\u7fa4 \uff1a859886087 \u60a8\u4e5f\u53ef\u4ee5\u5728\u8d21\u732e\u8005\u540d\u5355\u4e2d\u53c2\u770b\u6240\u6709\u53c2\u4e0e\u8be5\u9879\u76ee\u7684\u5f00\u53d1\u8005\u3002","title":"\u4f5c\u8005"},{"location":"#_9","text":"\u8bf7\u9605\u8bfb CONTRIBUTING.md \u67e5\u9605\u4e3a\u8be5\u9879\u76ee\u505a\u51fa\u8d21\u732e\u7684\u5f00\u53d1\u8005\u3002","title":"\u8d21\u732e\u8005"},{"location":"#_10","text":"\u8d21\u732e\u4f7f\u5f00\u6e90\u793e\u533a\u6210\u4e3a\u4e00\u4e2a\u5b66\u4e60\u3001\u6fc0\u52b1\u548c\u521b\u9020\u7684\u7edd\u4f73\u573a\u6240\u3002\u4f60\u6240\u4f5c\u7684\u4efb\u4f55\u8d21\u732e\u90fd\u662f \u975e\u5e38\u611f\u8c22 \u7684\u3002 Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request","title":"\u5982\u4f55\u53c2\u4e0e\u5f00\u6e90\u9879\u76ee"},{"location":"#_11","text":"\u8be5\u9879\u76ee\u7b7e\u7f72\u4e86MIT \u6388\u6743\u8bb8\u53ef\uff0c\u8be6\u60c5\u8bf7\u53c2\u9605 LICENSE","title":"\u7248\u6743\u8bf4\u660e"},{"location":"#_12","text":"\u8be5\u9879\u76ee\u4f7f\u7528Git\u8fdb\u884c\u7248\u672c\u7ba1\u7406\u3002\u60a8\u53ef\u4ee5\u5728repository\u53c2\u770b\u5f53\u524d\u53ef\u7528\u7248\u672c\u3002","title":"\u7248\u672c\u63a7\u5236"},{"location":"#_13","text":"Free Logo Design","title":"\u9e23\u8c22"},{"location":"CONTRIBUTING/","text":"Contribution Guide We welcome any contributions whether it's, Submitting feedback Fixing bugs Or implementing a new feature. Please read this guide before making any contributions. Submit Feedback The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels. Fix Bugs: You may look through the GitHub issues for bugs. Implement Features You may look through the GitHub issues for feature requests. Pull Requests (PR) Fork the repository and a create a new branch from the master branch. For bug fixes, add new tests and for new features please add changes to the documentation. Do a PR from your new branch to our master branch of the original Headliner repo. Documentation Make sure any new function or class you introduce has proper docstrings. Testing We use pytest for our testing. Make sure to write tests for any new feature and/or bug fixes. Main Contributor List We maintain a list of main contributors to appreciate all the contributions.","title":"Contribution"},{"location":"CONTRIBUTING/#contribution-guide","text":"We welcome any contributions whether it's, Submitting feedback Fixing bugs Or implementing a new feature. Please read this guide before making any contributions.","title":"Contribution Guide"},{"location":"CONTRIBUTING/#submit-feedback","text":"The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels.","title":"Submit Feedback"},{"location":"CONTRIBUTING/#fix-bugs","text":"You may look through the GitHub issues for bugs.","title":"Fix Bugs:"},{"location":"CONTRIBUTING/#implement-features","text":"You may look through the GitHub issues for feature requests.","title":"Implement Features"},{"location":"CONTRIBUTING/#pull-requests-pr","text":"Fork the repository and a create a new branch from the master branch. For bug fixes, add new tests and for new features please add changes to the documentation. Do a PR from your new branch to our master branch of the original Headliner repo.","title":"Pull Requests (PR)"},{"location":"CONTRIBUTING/#documentation","text":"Make sure any new function or class you introduce has proper docstrings.","title":"Documentation"},{"location":"CONTRIBUTING/#testing","text":"We use pytest for our testing. Make sure to write tests for any new feature and/or bug fixes.","title":"Testing"},{"location":"CONTRIBUTING/#main-contributor-list","text":"We maintain a list of main contributors to appreciate all the contributions.","title":"Main Contributor List"},{"location":"LICENSE/","text":"MIT License Copyright (c) 2019 SunYan Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"examples/advanced_nmt_example/","text":"Advanced Neural Machine Translation Example Install TensorFlow and also our package via PyPI pip install tensorflow-gpu == 2 .0.0 pip install headliner Download the German-English sentence pairs wget http://www.manythings.org/anki/deu-eng.zip unzip deu-eng.zip Create the dataset but only take a subset for faster training import io def create_dataset ( path , num_examples ): lines = io . open ( path , encoding = 'UTF-8' ) . read () . strip () . split ( ' \\n ' ) word_pairs = [[ w for w in l . split ( ' \\t ' )[: 2 ]] for l in lines [: num_examples ]] return zip ( * word_pairs ) eng , ger = create_dataset ( 'deu.txt' , 30000 ) data = list ( zip ( eng , ger )) Split the dataset into train and test from sklearn.model_selection import train_test_split train , test = train_test_split ( data , test_size = 100 ) Define custom preprocessing from headliner.preprocessing import Preprocessor preprocessor = Preprocessor ( lower_case = True ) train_prep = [ preprocessor ( t ) for t in train ] Fit custom tokenizers for input and target from tensorflow_datasets.core.features.text import SubwordTextEncoder from headliner.preprocessing import Vectorizer inputs_prep = [ t [ 0 ] for t in train_prep ] targets_prep = [ t [ 1 ] for t in train_prep ] tokenizer_input = SubwordTextEncoder . build_from_corpus ( inputs_prep , target_vocab_size = 2 ** 13 , reserved_tokens = [ preprocessor . start_token , preprocessor . end_token ]) tokenizer_target = SubwordTextEncoder . build_from_corpus ( targets_prep , target_vocab_size = 2 ** 13 , reserved_tokens = [ preprocessor . start_token , preprocessor . end_token ]) vectorizer = Vectorizer ( tokenizer_input , tokenizer_target ) 'vocab size input {}, target {}' . format ( vectorizer . encoding_dim , vectorizer . decoding_dim ) Start tensorboard %load_ext tensorboard %tensorboard -- logdir / tmp / summarizer_tensorboard Define the model and train it from headliner.model.summarizer_transformer import SummarizerTransformer from headliner.trainer import Trainer summarizer = SummarizerTransformer ( num_heads = 2 , feed_forward_dim = 1024 , num_layers = 1 , embedding_size = 64 , dropout_rate = 0.1 , max_prediction_len = 50 ) summarizer . init_model ( preprocessor , vectorizer ) trainer = Trainer ( steps_per_epoch = 250 , batch_size = 64 , model_save_path = '/tmp/summarizer_transformer' , tensorboard_dir = '/tmp/summarizer_tensorboard' , steps_to_log = 50 ) trainer . train ( summarizer , train , num_epochs = 10 , val_data = test ) Load best model and do some prediction best_summarizer = SummarizerTransformer . load ( '/tmp/summarizer_transformer' ) best_summarizer . predict ( 'Do you like robots?' ) Plot attention alignment for a prediction import tensorflow as tf import matplotlib.pyplot as plt def plot_attention_weights ( summarizer , pred_vectors , layer_name ): fig = plt . figure ( figsize = ( 16 , 8 )) input_text , _ = pred_vectors [ 'preprocessed_text' ] input_sequence = summarizer . vectorizer . encode_input ( input_text ) pred_sequence = pred_vectors [ 'predicted_sequence' ] attention = tf . squeeze ( pred_vectors [ 'attention_weights' ][ layer_name ]) for head in range ( attention . shape [ 0 ]): ax = fig . add_subplot ( 1 , 2 , head + 1 ) ax . matshow ( attention [ head ][: - 1 , :], cmap = 'viridis' ) fontdict = { 'fontsize' : 10 } ax . set_xticks ( range ( len ( input_sequence ))) ax . set_yticks ( range ( len ( pred_sequence ))) ax . set_ylim ( len ( pred_sequence ) - 1.5 , - 0.5 ) ax . set_xticklabels ( [ summarizer . vectorizer . decode_input ([ i ]) for i in input_sequence ], fontdict = fontdict , rotation = 90 ) ax . set_yticklabels ([ summarizer . vectorizer . decode_output ([ i ]) for i in pred_sequence ], fontdict = fontdict ) ax . set_xlabel ( 'Head {}' . format ( head + 1 )) plt . tight_layout () plt . show () pred_vectors = best_summarizer . predict_vectors ( 'Tom ran out of the house.' , '' ) plot_attention_weights ( best_summarizer , pred_vectors , 'decoder_layer1_block2' ) Continue training to improve the model and check the BLEU score from headliner.evaluation import BleuScorer bleu_scorer = BleuScorer ( tokens_to_ignore = [ preprocessor . start_token , preprocessor . end_token ]) trainer . train ( best_summarizer , train , num_epochs = 30 , val_data = test , scorers = { 'bleu' : bleu_scorer })","title":"Advanced Neural Machine Translation Example"},{"location":"examples/advanced_nmt_example/#advanced-neural-machine-translation-example","text":"","title":"Advanced Neural Machine Translation Example"},{"location":"examples/advanced_nmt_example/#install-tensorflow-and-also-our-package-via-pypi","text":"pip install tensorflow-gpu == 2 .0.0 pip install headliner","title":"Install TensorFlow and also our package via PyPI"},{"location":"examples/advanced_nmt_example/#download-the-german-english-sentence-pairs","text":"wget http://www.manythings.org/anki/deu-eng.zip unzip deu-eng.zip","title":"Download the German-English sentence pairs"},{"location":"examples/advanced_nmt_example/#create-the-dataset-but-only-take-a-subset-for-faster-training","text":"import io def create_dataset ( path , num_examples ): lines = io . open ( path , encoding = 'UTF-8' ) . read () . strip () . split ( ' \\n ' ) word_pairs = [[ w for w in l . split ( ' \\t ' )[: 2 ]] for l in lines [: num_examples ]] return zip ( * word_pairs ) eng , ger = create_dataset ( 'deu.txt' , 30000 ) data = list ( zip ( eng , ger ))","title":"Create the dataset but only take a subset for faster training"},{"location":"examples/advanced_nmt_example/#split-the-dataset-into-train-and-test","text":"from sklearn.model_selection import train_test_split train , test = train_test_split ( data , test_size = 100 )","title":"Split the dataset into train and test"},{"location":"examples/advanced_nmt_example/#define-custom-preprocessing","text":"from headliner.preprocessing import Preprocessor preprocessor = Preprocessor ( lower_case = True ) train_prep = [ preprocessor ( t ) for t in train ]","title":"Define custom preprocessing"},{"location":"examples/advanced_nmt_example/#fit-custom-tokenizers-for-input-and-target","text":"from tensorflow_datasets.core.features.text import SubwordTextEncoder from headliner.preprocessing import Vectorizer inputs_prep = [ t [ 0 ] for t in train_prep ] targets_prep = [ t [ 1 ] for t in train_prep ] tokenizer_input = SubwordTextEncoder . build_from_corpus ( inputs_prep , target_vocab_size = 2 ** 13 , reserved_tokens = [ preprocessor . start_token , preprocessor . end_token ]) tokenizer_target = SubwordTextEncoder . build_from_corpus ( targets_prep , target_vocab_size = 2 ** 13 , reserved_tokens = [ preprocessor . start_token , preprocessor . end_token ]) vectorizer = Vectorizer ( tokenizer_input , tokenizer_target ) 'vocab size input {}, target {}' . format ( vectorizer . encoding_dim , vectorizer . decoding_dim )","title":"Fit custom tokenizers for input and target"},{"location":"examples/advanced_nmt_example/#start-tensorboard","text":"%load_ext tensorboard %tensorboard -- logdir / tmp / summarizer_tensorboard","title":"Start tensorboard"},{"location":"examples/advanced_nmt_example/#define-the-model-and-train-it","text":"from headliner.model.summarizer_transformer import SummarizerTransformer from headliner.trainer import Trainer summarizer = SummarizerTransformer ( num_heads = 2 , feed_forward_dim = 1024 , num_layers = 1 , embedding_size = 64 , dropout_rate = 0.1 , max_prediction_len = 50 ) summarizer . init_model ( preprocessor , vectorizer ) trainer = Trainer ( steps_per_epoch = 250 , batch_size = 64 , model_save_path = '/tmp/summarizer_transformer' , tensorboard_dir = '/tmp/summarizer_tensorboard' , steps_to_log = 50 ) trainer . train ( summarizer , train , num_epochs = 10 , val_data = test )","title":"Define the model and train it"},{"location":"examples/advanced_nmt_example/#load-best-model-and-do-some-prediction","text":"best_summarizer = SummarizerTransformer . load ( '/tmp/summarizer_transformer' ) best_summarizer . predict ( 'Do you like robots?' )","title":"Load best model and do some prediction"},{"location":"examples/advanced_nmt_example/#plot-attention-alignment-for-a-prediction","text":"import tensorflow as tf import matplotlib.pyplot as plt def plot_attention_weights ( summarizer , pred_vectors , layer_name ): fig = plt . figure ( figsize = ( 16 , 8 )) input_text , _ = pred_vectors [ 'preprocessed_text' ] input_sequence = summarizer . vectorizer . encode_input ( input_text ) pred_sequence = pred_vectors [ 'predicted_sequence' ] attention = tf . squeeze ( pred_vectors [ 'attention_weights' ][ layer_name ]) for head in range ( attention . shape [ 0 ]): ax = fig . add_subplot ( 1 , 2 , head + 1 ) ax . matshow ( attention [ head ][: - 1 , :], cmap = 'viridis' ) fontdict = { 'fontsize' : 10 } ax . set_xticks ( range ( len ( input_sequence ))) ax . set_yticks ( range ( len ( pred_sequence ))) ax . set_ylim ( len ( pred_sequence ) - 1.5 , - 0.5 ) ax . set_xticklabels ( [ summarizer . vectorizer . decode_input ([ i ]) for i in input_sequence ], fontdict = fontdict , rotation = 90 ) ax . set_yticklabels ([ summarizer . vectorizer . decode_output ([ i ]) for i in pred_sequence ], fontdict = fontdict ) ax . set_xlabel ( 'Head {}' . format ( head + 1 )) plt . tight_layout () plt . show () pred_vectors = best_summarizer . predict_vectors ( 'Tom ran out of the house.' , '' ) plot_attention_weights ( best_summarizer , pred_vectors , 'decoder_layer1_block2' )","title":"Plot attention alignment for a prediction"},{"location":"examples/advanced_nmt_example/#continue-training-to-improve-the-model-and-check-the-bleu-score","text":"from headliner.evaluation import BleuScorer bleu_scorer = BleuScorer ( tokens_to_ignore = [ preprocessor . start_token , preprocessor . end_token ]) trainer . train ( best_summarizer , train , num_epochs = 30 , val_data = test , scorers = { 'bleu' : bleu_scorer })","title":"Continue training to improve the model and check the BLEU score"},{"location":"examples/bert_example/","text":"Advanced Neural Machine Translation Example Install TensorFlow and also our package via PyPI pip install tensorflow-gpu == 2 .0.0 pip install headliner Download the German-English sentence pairs wget http://www.manythings.org/anki/deu-eng.zip unzip deu-eng.zip Create the dataset but only take a subset for faster training import io def create_dataset ( path , num_examples ): lines = io . open ( path , encoding = 'UTF-8' ) . read () . strip () . split ( ' \\n ' ) word_pairs = [[ w for w in l . split ( ' \\t ' )[: 2 ]] for l in lines [: num_examples ]] return zip ( * word_pairs ) eng , ger = create_dataset ( 'deu.txt' , 30000 ) data = list ( zip ( eng , ger )) Split the dataset into train and test from sklearn.model_selection import train_test_split train , test = train_test_split ( data , test_size = 100 ) Define custom preprocessing from headliner.preprocessing import Preprocessor preprocessor = Preprocessor ( lower_case = True ) train_prep = [ preprocessor ( t ) for t in train ] Create custom tokenizers for input and target from tensorflow_datasets.core.features.text import SubwordTextEncoder from transformers import TFBertModel , BertTokenizer from headliner.preprocessing import Vectorizer targets_prep = [ t [ 1 ] for t in train_prep ] tokenizer_input = BertTokenizer . from_pretrained ( 'bert-base-uncased' ) tokenizer_target = SubwordTextEncoder . build_from_corpus ( targets_prep , target_vocab_size = 2 ** 13 , reserved_tokens = [ preprocessor . start_token , preprocessor . end_token ]) vectorizer = Vectorizer ( tokenizer_input , tokenizer_target ) 'vocab size input {}, target {}' . format ( vectorizer . encoding_dim , vectorizer . decoding_dim ) ### Start tensorboard %load_ext tensorboard %tensorboard --logdir /tmp/summarizer_tensorboard ### Define the model and train it `` ` python from headliner.model.summarizer_bert import SummarizerBert from headliner.trainer import Trainer # use pre-trained BERT embedding for the encoder and freeze it # for faster training summarizer = SummarizerBert ( num_heads = 2 , feed_forward_dim = 512 , num_layers_encoder = 1 , num_layers_decoder = 1 , bert_embedding_encoder = 'bert-base-uncased' , embedding_encoder_trainable = False , embedding_size_encoder = 768 , embedding_size_decoder = 64 , dropout_rate = 0.1 , max_prediction_len = 50 ) summarizer . init_model ( preprocessor , vectorizer ) trainer = Trainer ( steps_per_epoch = 250 , batch_size = 64 , model_save_path = '/tmp/summarizer_transformer' , tensorboard_dir = '/tmp/summarizer_tensorboard' , steps_to_log = 50 ) trainer . train ( summarizer , train , num_epochs = 10 , val_data = test ) Load best model and do some prediction best_summarizer = SummarizerTransformer . load ( '/tmp/summarizer_transformer' ) best_summarizer . predict ( 'Do you like robots?' ) Plot attention alignment for a prediction import tensorflow as tf import matplotlib.pyplot as plt def plot_attention_weights ( summarizer , pred_vectors , layer_name ): fig = plt . figure ( figsize = ( 16 , 8 )) input_text , _ = pred_vectors [ 'preprocessed_text' ] input_sequence = summarizer . vectorizer . encode_input ( input_text ) pred_sequence = pred_vectors [ 'predicted_sequence' ] attention = tf . squeeze ( pred_vectors [ 'attention_weights' ][ layer_name ]) for head in range ( attention . shape [ 0 ]): ax = fig . add_subplot ( 1 , 2 , head + 1 ) ax . matshow ( attention [ head ][: - 1 , :], cmap = 'viridis' ) fontdict = { 'fontsize' : 10 } ax . set_xticks ( range ( len ( input_sequence ))) ax . set_yticks ( range ( len ( pred_sequence ))) ax . set_ylim ( len ( pred_sequence ) - 1.5 , - 0.5 ) ax . set_xticklabels ( [ summarizer . vectorizer . decode_input ([ i ]) for i in input_sequence ], fontdict = fontdict , rotation = 90 ) ax . set_yticklabels ([ summarizer . vectorizer . decode_output ([ i ]) for i in pred_sequence ], fontdict = fontdict ) ax . set_xlabel ( 'Head {}' . format ( head + 1 )) plt . tight_layout () plt . show () pred_vectors = best_summarizer . predict_vectors ( 'Tom ran out of the house.' , '' ) plot_attention_weights ( best_summarizer , pred_vectors , 'decoder_layer1_block2' ) Continue training to improve the model and check the BLEU score from headliner.evaluation import BleuScorer bleu_scorer = BleuScorer ( tokens_to_ignore = [ preprocessor . start_token , preprocessor . end_token ]) trainer . train ( best_summarizer , train , num_epochs = 30 , val_data = test , scorers = { 'bleu' : bleu_scorer })","title":"Advanced Neural Machine Translation Example"},{"location":"examples/bert_example/#advanced-neural-machine-translation-example","text":"","title":"Advanced Neural Machine Translation Example"},{"location":"examples/bert_example/#install-tensorflow-and-also-our-package-via-pypi","text":"pip install tensorflow-gpu == 2 .0.0 pip install headliner","title":"Install TensorFlow and also our package via PyPI"},{"location":"examples/bert_example/#download-the-german-english-sentence-pairs","text":"wget http://www.manythings.org/anki/deu-eng.zip unzip deu-eng.zip","title":"Download the German-English sentence pairs"},{"location":"examples/bert_example/#create-the-dataset-but-only-take-a-subset-for-faster-training","text":"import io def create_dataset ( path , num_examples ): lines = io . open ( path , encoding = 'UTF-8' ) . read () . strip () . split ( ' \\n ' ) word_pairs = [[ w for w in l . split ( ' \\t ' )[: 2 ]] for l in lines [: num_examples ]] return zip ( * word_pairs ) eng , ger = create_dataset ( 'deu.txt' , 30000 ) data = list ( zip ( eng , ger ))","title":"Create the dataset but only take a subset for faster training"},{"location":"examples/bert_example/#split-the-dataset-into-train-and-test","text":"from sklearn.model_selection import train_test_split train , test = train_test_split ( data , test_size = 100 )","title":"Split the dataset into train and test"},{"location":"examples/bert_example/#define-custom-preprocessing","text":"from headliner.preprocessing import Preprocessor preprocessor = Preprocessor ( lower_case = True ) train_prep = [ preprocessor ( t ) for t in train ]","title":"Define custom preprocessing"},{"location":"examples/bert_example/#create-custom-tokenizers-for-input-and-target","text":"from tensorflow_datasets.core.features.text import SubwordTextEncoder from transformers import TFBertModel , BertTokenizer from headliner.preprocessing import Vectorizer targets_prep = [ t [ 1 ] for t in train_prep ] tokenizer_input = BertTokenizer . from_pretrained ( 'bert-base-uncased' ) tokenizer_target = SubwordTextEncoder . build_from_corpus ( targets_prep , target_vocab_size = 2 ** 13 , reserved_tokens = [ preprocessor . start_token , preprocessor . end_token ]) vectorizer = Vectorizer ( tokenizer_input , tokenizer_target ) 'vocab size input {}, target {}' . format ( vectorizer . encoding_dim , vectorizer . decoding_dim ) ### Start tensorboard %load_ext tensorboard %tensorboard --logdir /tmp/summarizer_tensorboard ### Define the model and train it `` ` python from headliner.model.summarizer_bert import SummarizerBert from headliner.trainer import Trainer # use pre-trained BERT embedding for the encoder and freeze it # for faster training summarizer = SummarizerBert ( num_heads = 2 , feed_forward_dim = 512 , num_layers_encoder = 1 , num_layers_decoder = 1 , bert_embedding_encoder = 'bert-base-uncased' , embedding_encoder_trainable = False , embedding_size_encoder = 768 , embedding_size_decoder = 64 , dropout_rate = 0.1 , max_prediction_len = 50 ) summarizer . init_model ( preprocessor , vectorizer ) trainer = Trainer ( steps_per_epoch = 250 , batch_size = 64 , model_save_path = '/tmp/summarizer_transformer' , tensorboard_dir = '/tmp/summarizer_tensorboard' , steps_to_log = 50 ) trainer . train ( summarizer , train , num_epochs = 10 , val_data = test )","title":"Create custom tokenizers for input and target"},{"location":"examples/bert_example/#load-best-model-and-do-some-prediction","text":"best_summarizer = SummarizerTransformer . load ( '/tmp/summarizer_transformer' ) best_summarizer . predict ( 'Do you like robots?' )","title":"Load best model and do some prediction"},{"location":"examples/bert_example/#plot-attention-alignment-for-a-prediction","text":"import tensorflow as tf import matplotlib.pyplot as plt def plot_attention_weights ( summarizer , pred_vectors , layer_name ): fig = plt . figure ( figsize = ( 16 , 8 )) input_text , _ = pred_vectors [ 'preprocessed_text' ] input_sequence = summarizer . vectorizer . encode_input ( input_text ) pred_sequence = pred_vectors [ 'predicted_sequence' ] attention = tf . squeeze ( pred_vectors [ 'attention_weights' ][ layer_name ]) for head in range ( attention . shape [ 0 ]): ax = fig . add_subplot ( 1 , 2 , head + 1 ) ax . matshow ( attention [ head ][: - 1 , :], cmap = 'viridis' ) fontdict = { 'fontsize' : 10 } ax . set_xticks ( range ( len ( input_sequence ))) ax . set_yticks ( range ( len ( pred_sequence ))) ax . set_ylim ( len ( pred_sequence ) - 1.5 , - 0.5 ) ax . set_xticklabels ( [ summarizer . vectorizer . decode_input ([ i ]) for i in input_sequence ], fontdict = fontdict , rotation = 90 ) ax . set_yticklabels ([ summarizer . vectorizer . decode_output ([ i ]) for i in pred_sequence ], fontdict = fontdict ) ax . set_xlabel ( 'Head {}' . format ( head + 1 )) plt . tight_layout () plt . show () pred_vectors = best_summarizer . predict_vectors ( 'Tom ran out of the house.' , '' ) plot_attention_weights ( best_summarizer , pred_vectors , 'decoder_layer1_block2' )","title":"Plot attention alignment for a prediction"},{"location":"examples/bert_example/#continue-training-to-improve-the-model-and-check-the-bleu-score","text":"from headliner.evaluation import BleuScorer bleu_scorer = BleuScorer ( tokens_to_ignore = [ preprocessor . start_token , preprocessor . end_token ]) trainer . train ( best_summarizer , train , num_epochs = 30 , val_data = test , scorers = { 'bleu' : bleu_scorer })","title":"Continue training to improve the model and check the BLEU score"},{"location":"examples/nmt_example/","text":"Neural Machine Translation Example Install TensorFlow and also our package via PyPI pip install tensorflow-gpu == 2 .0.0 pip install headliner Download the German-English sentence pairs wget http://www.manythings.org/anki/deu-eng.zip unzip deu-eng.zip Create the dataset but only take a subset for faster training import io def create_dataset ( path , num_examples ): lines = io . open ( path , encoding = 'UTF-8' ) . read () . strip () . split ( ' \\n ' ) word_pairs = [[ w for w in l . split ( ' \\t ' )[: 2 ]] for l in lines [: num_examples ]] return zip ( * word_pairs ) eng , ger = create_dataset ( 'deu.txt' , 30000 ) data = list ( zip ( eng , ger )) Split the dataset into train and test from sklearn.model_selection import train_test_split train , test = train_test_split ( data , test_size = 100 ) Define the model and train it from headliner.trainer import Trainer from headliner.model.summarizer_attention import SummarizerAttention summarizer = SummarizerAttention ( lstm_size = 1024 , embedding_size = 256 ) trainer = Trainer ( batch_size = 64 , steps_per_epoch = 100 , steps_to_log = 20 , max_output_len = 10 , model_save_path = '/tmp/summarizer' ) trainer . train ( summarizer , train , num_epochs = 10 , val_data = test ) Do some prediction summarizer . predict ( 'How are you?' )","title":"Neural Machine Translation Example"},{"location":"examples/nmt_example/#neural-machine-translation-example","text":"","title":"Neural Machine Translation Example"},{"location":"examples/nmt_example/#install-tensorflow-and-also-our-package-via-pypi","text":"pip install tensorflow-gpu == 2 .0.0 pip install headliner","title":"Install TensorFlow and also our package via PyPI"},{"location":"examples/nmt_example/#download-the-german-english-sentence-pairs","text":"wget http://www.manythings.org/anki/deu-eng.zip unzip deu-eng.zip","title":"Download the German-English sentence pairs"},{"location":"examples/nmt_example/#create-the-dataset-but-only-take-a-subset-for-faster-training","text":"import io def create_dataset ( path , num_examples ): lines = io . open ( path , encoding = 'UTF-8' ) . read () . strip () . split ( ' \\n ' ) word_pairs = [[ w for w in l . split ( ' \\t ' )[: 2 ]] for l in lines [: num_examples ]] return zip ( * word_pairs ) eng , ger = create_dataset ( 'deu.txt' , 30000 ) data = list ( zip ( eng , ger ))","title":"Create the dataset but only take a subset for faster training"},{"location":"examples/nmt_example/#split-the-dataset-into-train-and-test","text":"from sklearn.model_selection import train_test_split train , test = train_test_split ( data , test_size = 100 )","title":"Split the dataset into train and test"},{"location":"examples/nmt_example/#define-the-model-and-train-it","text":"from headliner.trainer import Trainer from headliner.model.summarizer_attention import SummarizerAttention summarizer = SummarizerAttention ( lstm_size = 1024 , embedding_size = 256 ) trainer = Trainer ( batch_size = 64 , steps_per_epoch = 100 , steps_to_log = 20 , max_output_len = 10 , model_save_path = '/tmp/summarizer' ) trainer . train ( summarizer , train , num_epochs = 10 , val_data = test )","title":"Define the model and train it"},{"location":"examples/nmt_example/#do-some-prediction","text":"summarizer . predict ( 'How are you?' )","title":"Do some prediction"}]}